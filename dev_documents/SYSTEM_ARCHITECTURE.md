# SYSTEM_ARCHITECTURE.md

## 1. Summary

The Autonomous Development Environment (AC-CDD) represents a paradigm shift in software engineering, moving from a tool-assisted model to a truly AI-native, autonomous framework. It is designed to automate the entire software development lifecycle, from the initial seed of an idea to a fully tested, high-quality, and deployable product. The core philosophy of AC-CDD is to establish a "Committee of AI Agents," a collaborative ecosystem where specialized AI agents, each with a distinct role, work in concert to build and maintain software. This approach is a direct response to the escalating complexity and persistent inefficiencies inherent in modern development workflows. Today's developers are burdened with a significant cognitive load that extends far beyond pure problem-solving; they must navigate intricate toolchains, manually triage bugs, enforce coding standards, and write vast amounts of boilerplate code. AC-CDD addresses these challenges head-on by automating these laborious tasks, thereby liberating human engineers to focus on what they do best: innovation, strategic thinking, and creative design.

The system is built upon a novel methodology: AI-native Cycle-Based Contract-Driven Development (AC-CDD). This methodology provides a structured and rigorous framework for the AI agents. A "contract," in the form of detailed technical specifications (`SPEC.md`) and explicit User Acceptance Tests (`UAT.md`), is automatically generated by an AI Architect at the outset of each development cycle. This contract serves as the non-negotiable source of truth for the entire cycle, ensuring that all subsequent implementation work is precisely aligned with clearly defined, verifiable goals. The project is decomposed into these discrete, self-contained cycles, each delivering a tangible and testable piece of functionality. This iterative, cycle-based approach not only provides a steady rhythm of progress but also facilitates a process of continuous integration and validation, drastically reducing the risk of large-scale integration failures.

A foundational pillar of the AC-CDD architecture is its unwavering commitment to security and reproducibility, embodied by its fully remote, sandboxed execution model. Every operation that involves code generation, modification, compilation, or testing is executed within a secure, ephemeral E2B sandbox. This architectural choice completely isolates the development process from the user's local machine, providing a pristine, reproducible environment for every single run. This eradicates the notorious "it works on my machine" class of problems, ensures that dependencies are managed cleanly, and guarantees that test results are a reliable measure of code quality. Furthermore, this sandboxed approach provides a critical security layer, as any code generated by the AI agents—which must be treated as potentially untrusted—is executed in an environment where it cannot access the local filesystem or network resources. The AC-CDD framework seamlessly manages the synchronization of files between the user's local repository and the remote sandbox, creating a user experience that is both powerful and secure. The ultimate vision is a system that does not just write code, but reasons about it, critiques it, and refines it, pioneering a future where software engineering is a collaboration between human creativity and artificial intelligence.

## 2. System Design Objectives

The design of the AC-CDD is underpinned by a set of core objectives that collectively aim to forge a more efficient, reliable, and intelligent software development paradigm. These objectives serve as the guiding principles for the architecture and functionality of the entire system.

**1. Maximise Automation, Minimise Human Intervention:** The foremost objective is to achieve a state of near-complete automation across the development lifecycle. This extends beyond simple code generation. The goal is to automate the high-level cognitive processes traditionally performed by human engineers: interpreting ambiguous requirements, formulating a coherent system architecture, decomposing problems into actionable tasks, writing clean and idiomatic code, developing comprehensive test suites, performing rigorous code reviews, and debugging failures. The ideal interaction model is one where the human provides the high-level "what" (the raw requirements), and the AC-CDD system autonomously handles the "how" (the entire implementation process). This radical reduction in required human intervention is designed to produce a step-change in development velocity and to free human talent to focus on business strategy, user experience, and long-term product vision rather than the minutiae of implementation.

**2. Enforce Quality and Security as Non-Negotiable Gates:** In many traditional workflows, quality and security are treated as checks to be performed late in the cycle, or worse, as afterthoughts. AC-CDD fundamentally inverts this model by integrating quality and security as foundational, non-negotiable gates at every stage. The "Committee of Auditors" is the institutional embodiment of this principle. It is an automated, multi-agent review process that scrutinizes every line of generated code from multiple perspectives. One agent might act as a security expert, using static analysis techniques to hunt for common vulnerabilities like injection flaws or improper exception handling. Another might act as a performance engineer, flagging inefficient algorithms or memory management issues. A third could be a "style guardian," ensuring the code adheres strictly to project-specific conventions. This multi-faceted, automated auditing process, which occurs before any code is considered "done," is designed to be far more rigorous and consistent than manual peer review, ensuring that every piece of code that completes a cycle is not just functionally correct, but also robust, secure, and maintainable.

**3. Guarantee Environmental Consistency and Reproducibility:** The sandboxed execution model is a cornerstone of the system's reliability. The objective is to completely eliminate the environmental variables that plague so many development projects. By ensuring that every single operation—from installing dependencies to running tests—occurs in a pristine, ephemeral E2B sandbox, AC-CDD guarantees a perfectly consistent and reproducible environment. This means a test that passes once will pass every time, and code that works in the sandbox is guaranteed to work in an identical production environment. This objective directly addresses issues of dependency hell, configuration drift, and the "it works on my machine" syndrome. It also simplifies the onboarding process for new developers or team members, as there is no complex local environment to configure. The environment is defined as code and instantiated on demand.

**4. Foster Modularity, Extensibility, and Adaptability:** The world of AI and software development is in a constant state of flux. New models, tools, and best practices emerge weekly. The AC-CDD system is therefore designed to be inherently modular and extensible to avoid obsolescence. The separation of concerns between the Orchestrator, the Agents, the Tools, and the Execution Environment is deliberate and strict. Each agent is a pluggable component. A user could, through a simple configuration change, replace the default Gemini-based Auditor with a more specialized static analysis tool or a new, more powerful LLM. The `LangGraph` orchestrator defines the "how" of the workflow, but the agents are the "who." This architecture allows the system to adapt and evolve. If a new, superior code generation model becomes available, the `CoderAgent` can be updated to use its client without requiring any changes to the orchestration logic. This ensures the long-term viability of the framework and allows it to incorporate the best-of-breed tools as they become available.

## 3. System Architecture

The AC-CDD framework is architected as a layered system, ensuring a clear separation of concerns and promoting modularity. This design allows for independent development, testing, and evolution of each layer. The four primary layers are the Orchestration Layer, the Agent Layer, the Tooling Layer, and the Execution Layer.

```mermaid
graph TD
    A[User CLI] --> B{Orchestration Layer (LangGraph)};
    B --> C1[Architect Agent (Jules)];
    B --> C2[Coder Agent (Jules)];
    B --> C3[Auditor Agent (Fast Model)];
    B --> C4[QA Analyst Agent (Fast Model)];

    C1 --> D1[Documentation Tools];
    C2 --> D2[Code Generation & Modification Tools];
    C3 --> D2;
    C4 --> D3[Test Execution & Analysis Tools];

    subgraph "Agent Layer"
        C1; C2; C3; C4;
    end

    subgraph "Tooling Layer"
        D1; D2; D3;
    end

    D1 --> E[Local Filesystem];
    D2 --> F[E2B Sandbox];
    D3 --> F;

    F -- File Sync --> E;

    style F fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

**1. Orchestration Layer (LangGraph):**
This is the central nervous system of the AC-CDD framework, implemented using `LangGraph`. It functions as a sophisticated, stateful orchestrator that manages the entire lifecycle of a development cycle. It is not merely a script that runs tasks in sequence; it is a true state machine that directs the flow of control based on the outcomes of the tasks performed by the agents. The graph defines the nodes (the agents and their actions) and the edges (the transitions between states). For instance, a critical part of its logic is the conditional edge after the `TESTING` node: if the tests pass, the graph transitions to the `AUDITING` state; if they fail, the graph intelligently routes back to the `CODING` state, providing the Coder Agent with the failure logs as new context. This layer is also responsible for managing the state of the workflow, persisting information such as the current iteration count, the list of modified files, and the results of tests and audits. This persistent state is crucial for enabling the iterative fixing loop and for providing a clear audit trail of the entire process.

**2. Agent Layer:**
This layer comprises the specialized AI agents that form the "Committee of AI Agents." Each agent is an independent component with a clearly defined role and set of capabilities, powered by a specific class of Large Language Model best suited for its task.
- **Architect Agent (Jules):** This agent is the master planner. It is invoked at the very beginning of a project by the `gen-cycles` command. It utilizes a powerful, large-context model like Jules to perform the high-level reasoning required for system design. It ingests unstructured requirements from `ALL_SPEC.md` and synthesizes them into a complete, structured, and formal architectural blueprint, including the `SYSTEM_ARCHITECTURE.md` and the cycle-by-cycle `SPEC.md` and `UAT.md` files.
- **Coder Agent (Jules):** This is the primary implementer. Operating exclusively within the secure sandbox, it takes the "contract" for a given cycle (`SPEC.md`) and translates it into functional source code. It has access to a suite of tools for interacting with the file system within the sandbox. Crucially, this agent also plays the role of the "Fixer." In later iterations of the workflow loop, it receives test failure logs or audit reports and is tasked with debugging and correcting its own previously generated code.
- **Auditor Agent (Fast Model):** This agent acts as the guardian of code quality. It uses a fast, efficient, and cost-effective LLM (like Gemini Flash or Claude Haiku) that is optimized for analytical and critical tasks. It performs a rigorous static analysis of the code, checking for a wide range of issues including security vulnerabilities (e.g., SQL injection, insecure deserialization), logical errors, performance bottlenecks, and deviations from project-specific coding standards. Its output is a structured report of issues, which serves as a critical quality gate.
- **QA Analyst Agent (Fast Model):** This agent is the automated tester. Its responsibility is to verify that the generated code behaves as specified in the `UAT.md` contract. It does this by executing the project's test suite (e.g., `pytest`, `jest`) within the sandbox. It captures the results and determines whether the implementation meets the functional requirements. It provides a definitive pass/fail verdict that determines whether the workflow can proceed.

**3. Tooling Layer:**
This layer provides the concrete, non-AI capabilities that the agents use to perform their tasks. These tools are exposed to the agents as functions they can call.
- **Documentation Tools:** A set of utilities for interacting with the local filesystem, primarily used by the Architect Agent. This includes functions for creating directories and writing the generated Markdown and JSON files.
- **Code Generation & Modification Tools:** These are the Coder Agent's primary interface with the sandbox. They include functions like `write_file(path, content)`, `read_file(path)`, `list_files(directory)`, and a more advanced `patch_file(path, diff)` function.
- **Test Execution & Analysis Tools:** This is a wrapper around shell command execution inside the sandbox, used by the QA Analyst. It allows the agent to run commands like `pytest` and captures the `stdout`, `stderr`, and `exit code` in a structured format.

**4. Execution Layer:**
This layer comprises the environments where the work is actually done.
- **Local Filesystem:** The user's local machine, which holds the definitive source of truth for the project's code and documentation. The process starts here (reading requirements) and ends here (writing the final, approved code).
- **E2B Sandbox:** A secure, isolated, and ephemeral cloud-based environment. At the start of a `run-cycle`, the relevant parts of the repository (typically the `src/` directory) are synchronized to the sandbox. All code generation, dependency installation, compilation, and testing occur within this pristine environment. At the end of a successful run, the modified files are synchronized back to the local filesystem. This ensures safety, security, and perfect reproducibility.

## 4. Design Architecture

The internal software design of the AC-CDD framework prioritizes modularity, testability, and a clean separation of concerns, ensuring the system is both robust and maintainable.

**1. File Structure:**
The project's codebase is logically organized to reflect the architectural layers:
- `dev_src/ac_cdd_core/`: This is the main package containing the core logic of the AC-CDD tool.
  - `cli/`: This directory houses the user-facing command-line interface, built with `Typer`. It is responsible for parsing commands (`init`, `gen-cycles`, `run-cycle`), validating arguments, and invoking the appropriate backend services or orchestrators. It is the "thin" entry point to the system.
  - `agents/`: Each agent resides in its own module within this directory (e.g., `architect.py`, `coder.py`). This contains the core agentic logic, including prompt engineering, interaction with the LLM clients, and the orchestration of tool use.
  - `services/`: This directory contains stateless, reusable services that provide the heavy lifting for various tasks. Key services include:
    - `sandbox.py`: The `SandboxRunner` class, which encapsulates all logic for interacting with the E2B sandbox (lifecycle management, file synchronization, command execution).
    - `jules.py`: The `JulesClient`, responsible for all communication with the Jules API.
    - `git.py`: The `GitManager`, which abstracts away `git` command-line operations.
  - `graph/`: This module defines the `LangGraph` state machine. It contains the definitions for the graph's state (`CycleState`), the node functions that invoke the agents, and the conditional edges that control the workflow's logic.
  - `prompts/`: To keep logic and instructions separate, all system prompts for the agents are stored as external `.md` files in this directory. This allows for easy tuning of agent behavior without changing Python code.
  - `config.py`: The central point for application configuration. It uses `pydantic-settings` to load and validate settings from `.env` and `ac_cdd_config.py`.
- `src/`: This directory is intentionally kept separate and is reserved for the code of the project the user is building with the AC-CDD tool. The agents operate on the contents of this directory.
- `dev_documents/`: This directory contains all documentation, both user-provided (`ALL_SPEC.md`) and agent-generated (`SYSTEM_ARCHITECTURE.md`, `CYCLE{xx}/`).

**2. Core Classes and Data Models:**
- **`CycleState` (TypedDict):** This is the central data structure that is passed between nodes in the `LangGraph`. It is a dictionary-like object that holds the entire state of the current workflow, including the `cycle_id`, the `iteration_count`, lists of modified files, and structured `TestResult` and `AuditReport` objects.
- **`SandboxRunner`:** A key service class abstracting all E2B interactions. It provides a clean, high-level API (e.g., `start()`, `sync_to_sandbox()`, `execute_command()`, `close()`) that hides the underlying complexities of the `e2b` SDK. It is designed to be used as a context manager to ensure the sandbox is always properly shut down.
- **`JulesClient` / `FastModelClient`:** These classes are responsible for communication with the different LLM APIs. They handle the specifics of authentication, constructing the correct HTTP request payloads, and parsing the responses. This decouples the agents from the specific details of the LLM provider's API.
- **Pydantic Models (`TestResult`, `AuditReport`):** To ensure data consistency and enable robust error handling, the outputs of the QA and Auditor agents are defined as Pydantic models. For example, the `AuditReport` is a model containing a list of `AuditIssue` objects. This allows the `LangGraph` to reliably inspect the results and make decisions, and it prevents unstructured string parsing within the core orchestration logic.

**3. Data Flow Example (`run-cycle`):**
1.  User runs `uv run manage.py run-cycle --id 01`.
2.  The `CLI` invokes the `LangGraph` orchestrator, passing the cycle ID.
3.  The `Graph`'s first node, `SETUP_SANDBOX`, is executed. It instantiates the `SandboxRunner`, starts a new E2B sandbox, and syncs the current `src/` directory to it.
4.  The `Graph` transitions to the `CODING` state. The `CoderAgent` is invoked. It reads the `SPEC.md` for Cycle 01, and through a series of interactions with the `JulesClient` and the `SandboxRunner`'s file tools, it creates new code inside the sandbox.
5.  The `Graph` transitions to `TESTING`. The `QAAnalystAgent` is invoked. It uses `SandboxRunner.execute_command('pytest')` to run the tests. The results are captured in a `TestResult` object and saved to the `CycleState`.
6.  A conditional edge inspects the `TestResult`. Let's say a test fails. The edge directs the graph back to the `CODING` state.
7.  The `CoderAgent` is invoked again. This time, its prompt includes the original `SPEC.md` *and* the `stderr` from the `TestResult`. It generates a fix.
8.  The process repeats. This time, the tests pass. The conditional edge directs the graph to the `AUDITING` state.
9.  The `AuditorAgent` reads the modified files, gets a review from the `FastModelClient`, and finds no critical issues.
10. The `Graph` transitions to the `SYNC_BACK` state. The `SandboxRunner` archives the modified `src/` directory from the sandbox and overwrites the local one.
11. The `Graph` finishes. The `CLI` then uses the `GitManager` to commit the newly updated files to the feature branch.

## 5. Implementation Plan

The development of the AC-CDD framework is planned across five sequential, iterative cycles. Each cycle delivers a significant, functional piece of the final system, allowing for incremental testing and validation.

**Cycle 01: Core CLI, Initialization, and Configuration**
This foundational cycle focuses on the user's first experience and the basic scaffolding of the application. It's about building a usable, if limited, tool. The primary deliverable is a robust `init` command.
- **Features:**
  - A `Typer`-based CLI entry point (`manage.py`).
  - The `init` command will perform prerequisite checks, verifying that essential tools like `git`, `gh`, and `uv` are installed. This proactive checking prevents frustrating failures later on.
  - An interactive wizard within the `init` command will guide the user through creating their `.env` file from a template (`.env.example`), ensuring all necessary API keys are present.
  - A centralized configuration system using `pydantic-settings` will be implemented to load and validate all settings for the application.
  - A `ConsolePresenter` class using the `rich` library will be created to ensure all user-facing output is clean, colored, and consistently formatted.

**Cycle 02: Architect Agent (`gen-cycles` command)**
This cycle introduces the first layer of AI intelligence, automating the project's entire design phase. The deliverable is a `gen-cycles` command that turns raw requirements into a complete architectural plan.
- **Features:**
  - The `gen-cycles` CLI command.
  - A `JulesClient` service to handle all communication with the Jules API.
  - The `ArchitectAgent`, which will read the user's high-level requirements from `dev_documents/ALL_SPEC.md` and a master instruction prompt.
  - The agent will generate the four key architectural documents: `SYSTEM_ARCHITECTURE.md`, `CYCLE{xx}/SPEC.md`, `CYCLE{xx}/UAT.md`, and `plan_status.json`.
  - A simple `LangGraph` will be used to orchestrate this single-step agentic workflow.

**Cycle 03: Coder Agent and Sandbox Execution (`run-cycle` command)**
This cycle builds the core engine of the system: the ability to write and execute code in a secure environment. The `run-cycle` command becomes functional for the first time.
- **Features:**
  - The `run-cycle --id <cycle_id>` CLI command.
  - The critical `SandboxRunner` service, encapsulating all interactions with the E2B sandbox (lifecycle, file sync, command execution).
  - The `CoderAgent`, which takes a `SPEC.md` file and uses the `JulesClient` to generate code, operating exclusively through tools that interact with the sandbox.
  - The `LangGraph` for `run-cycle` will be created with an initial linear sequence: setup sandbox, run coder, sync back.

**Cycle 04: Auditor and QA Agent Integration**
This cycle layers the essential quality assurance processes on top of the code generation engine. The focus is on validation and reporting.
- **Features:**
  - A `FastModelClient` for interacting with cost-effective, analytical LLMs like Gemini.
  - The `QAAnalystAgent`, which uses the `SandboxRunner` to execute the test suite (e.g., `pytest`) within the sandbox and parse the results.
  - The `AuditorAgent`, which reads modified code from the sandbox and uses the `FastModelClient` to perform a static analysis for quality and security.
  - The `run-cycle` `LangGraph` will be extended to include `TESTING` and `AUDITING` states. Conditional logic will be added to halt the workflow and report errors if either of these quality gates fails.
  - Pydantic models (`TestResult`, `AuditReport`) will be introduced to structure the data flowing between these new states.

**Cycle 05: Iterative Fixing and Full Orchestration**
This final cycle "closes the loop," transforming the system from a linear executor into a truly autonomous, self-correcting agent.
- **Features:**
  - The `run-cycle` `LangGraph` will be re-architected to support a feedback loop. If the `TESTING` or `AUDITING` states fail, the graph will transition back to the `CODING` state.
  - The `CycleState` will be enhanced to carry the failure context (test logs or audit reports) back to the `CoderAgent`.
  - The `CoderAgent`'s prompt logic will be upgraded to handle this "fixing" context, instructing it to correct the issues rather than starting from scratch.
  - An iteration counter and a maximum iteration limit will be added to the graph's state to prevent infinite loops.
  - A `GitManager` service will be implemented. The `run-cycle` command will be updated to use it to create a feature branch at the start of a run and to commit the final, validated code at the end, fully integrating the autonomous workflow with standard development best practices.

## 6. Test Strategy

The test strategy for the AC-CDD framework is designed to be comprehensive, covering all layers of the architecture to ensure the tool itself is reliable, robust, and correct. The strategy is divided into three pillars: Unit Testing, Integration Testing, and End-to-End (E2E) Testing.

**Unit Testing:**
The goal of unit testing is to verify the correctness of each individual component (a single class or function) in complete isolation. This is achieved through extensive use of mocking.
- **Services:** Each method in a service will be tested independently. For the `GitManager`, the `subprocess.run` function will be mocked, and tests will assert that the correct `git` commands are being constructed. For the `JulesClient`, the `httpx` library will be mocked to simulate various API responses (success, authentication failure, server error) and verify that the client handles them correctly.
- **Agents:** The logic of each agent will be tested by providing it with mocked service dependencies. For the `AuditorAgent`, it will be given a mocked `FastModelClient` that returns a canned review string. The test will then assert that the agent's parsing logic correctly transforms that string into a structured `AuditReport` Pydantic model. The `CoderAgent`'s prompt construction logic will be tested by feeding it different `CycleState` objects and asserting that the generated prompt correctly reflects the current task (e.g., "implement" vs. "fix").
- **CLI:** The `Typer` `CliRunner` will be used to test the command-line interface. The services that the commands call will be mocked. These tests will verify that the CLI correctly parses arguments, invokes the right backend logic, and displays the expected messages to the user.

**Integration Testing:**
Integration tests are designed to verify that different components of the system work together as expected. These tests involve fewer mocks than unit tests.
- **Agent-Tool Integration:** These tests will verify that an agent can correctly use the tools provided by a service. For example, an integration test for the `CoderAgent` would mock the `JulesClient` but use a real `SandboxRunner` (connected to a real E2B sandbox). The test would simulate an LLM response instructing the agent to write a file and then verify, using the `SandboxRunner`'s read capabilities, that the file was actually written correctly inside the sandbox.
- **LangGraph Flow:** The logic of the `LangGraph` itself will be tested in an integrated manner. The nodes (which call the agents) will be mocked to return predefined results (e.g., a failing `TestResult`). The test will then invoke the entire graph and assert that it follows the correct path based on these results, for example, verifying that a failing test result correctly routes the graph back to the `CODING` node.

**End-to-End (E2E) Testing:**
E2E tests simulate the full user workflow from the command line down to the final output, with minimal mocking. These are the most comprehensive and valuable tests.
- **`gen-cycles` E2E Test:** This test will run `uv run manage.py gen-cycles` with a sample `ALL_SPEC.md`. It will mock the `JulesClient` to return a predefined string representing the full set of architectural documents. The test will then assert that all the expected files (`SYSTEM_ARCHITECTURE.md`, `CYCLE01/SPEC.md`, etc.) are created on the actual filesystem with the correct content.
- **`run-cycle` E2E Test (Capstone Test):** This is the most critical test for the entire system, primarily developed in Cycle 05. It will test the full self-healing loop.
  1.  A temporary local `git` repository will be created.
  2.  The `run-cycle` command will be invoked.
  3.  The LLM clients (`JulesClient`, `FastModelClient`) will be mocked.
  4.  The `JulesClient` will be configured to first return buggy code.
  5.  The `SandboxRunner` will connect to a **real E2B sandbox**.
  6.  The test suite (`pytest`) will run in the sandbox and fail.
  7.  The test will assert that the `LangGraph` loops back.
  8.  The `JulesClient` mock will then be configured to return fixed code.
  9.  The tests will run again and pass. The mocked Auditor will also pass.
  10. The test will finally assert that a feature branch has been created in the temporary git repository and that a single commit has been made containing the **final, correct** code. This single test validates the entire orchestrated workflow.
