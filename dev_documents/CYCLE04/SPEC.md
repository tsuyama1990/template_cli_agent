# Specification: Cycle 4 - Active Learning & Simulation

**Version:** 1.0.0
**Status:** Final

## 1. Summary

Cycle 4 marks a pivotal evolution for the MLIP-AutoPipe project, transforming it from a linear, one-shot pipeline into a dynamic, intelligent, and self-improving system. The primary objective of this cycle is to implement the active learning loop by developing `Module E: Simulation Engine`. In the previous cycles, we built a system that could generate an initial MLIP. This cycle will give that MLIP a purpose: to be used in a real simulation. More importantly, it will build the crucial feedback mechanism that allows the potential to learn from its own mistakes. The core goal is to create a system that can start with a partially trained potential, run a simulation, detect when and where the potential is failing, and automatically trigger the re-training pipeline to fix the deficiencies.

The scope of this cycle is to implement an "on-the-fly" (OTF) uncertainty monitoring and re-training workflow. We will integrate a standard simulation engine, such as LAMMPS, and configure it to use the MLIPs generated by our pipeline. The key innovation will be the development of an uncertainty quantification (UQ) mechanism. During an MD simulation, this UQ component will analyze the atomic configurations at each timestep and calculate a score representing the model's confidence in its prediction. If this uncertainty score exceeds a predefined threshold, it signifies that the model is operating in an "extrapolation" regime where its predictions are unreliable.

When such an event is triggered, the simulation will be automatically paused. The high-uncertainty atomic configuration will be extracted and sent back to the beginning of the pipeline (`Module B` or `C`) to be labeled with DFT and incorporated into the training data. The MLIP will then be re-trained, and the simulation will resume with the improved potential. This "OTF" loop is the cornerstone of the active learning strategy, ensuring that computational effort is focused precisely on the areas of the potential energy surface that the model does not yet understand. By the end of this cycle, MLIP-AutoPipe will be a truly adaptive learning machine, capable of refining its own potential to achieve the robustness required for production-level scientific simulations. This will enable the system to tackle more complex and dynamic problems, where the relevant atomic configurations are not known in advance. The successful completion of this cycle will elevate the project from a simple automation tool to a sophisticated, intelligent agent for materials discovery.

## 2. System Architecture

The architecture for Cycle 4 is the most complex yet, as it introduces a feedback loop that connects the end of the pipeline back to its beginning. The new `Module E: Simulation Engine` becomes the primary consumer of the trained MLIP, and its output (high-uncertainty structures) becomes a new source of input data for the labeling and training modules.

**Architectural Placement and Data Flow:**
The new architecture is no longer a simple linear sequence but a cycle.
1.  **Initial Training:** The pipeline begins as before, proceeding through Modules A, B, C, and D to produce an initial, "generation 0" MLIP.
2.  **Simulation and Monitoring:** This MLIP is then passed to the new `Module E: Simulation Engine`. The engine starts a production simulation (e.g., an MD run to simulate annealing or diffusion). At every step, it performs two tasks: it advances the simulation and it calculates the uncertainty of the MLIP's prediction for the current configuration.
3.  **Uncertainty Trigger:** The system continuously compares the uncertainty score to a threshold defined in the configuration. For most of the simulation, the score will be low, and the simulation continues.
4.  **Feedback Loop:** If the uncertainty threshold is breached:
    *   The simulation is paused.
    *   The current atomic structure (the one that caused the high uncertainty) is extracted. A key architectural decision is to use "periodic embedding," where the structure is extracted as a small, periodic cell to avoid artificial surface effects.
    *   This single structure is then sent back into the pipeline. It is added to the database of structures that require DFT labeling.
5.  **Re-training and Resumption:**
    *   The orchestrator triggers a re-training run. `Module C: Labeling Engine` is called to calculate the DFT properties for the new structure.
    *   `Module D: Training Engine` is then run again, this time using the entire previous dataset *plus* the new high-uncertainty point. This produces an improved "generation 1" MLIP.
    *   The simulation in `Module E` is then resumed from where it was paused, but now using the newly refined potential.

This entire process constitutes one active learning cycle. The simulation will continue, potentially being paused multiple times, until it completes its target simulation time. The final result is both the scientific output of the simulation and a highly robust, "simulation-hardened" MLIP.

**Internal Architecture of `Module E: Simulation Engine`:**
*   **A Top-Level `SimulationEngine` Class:** This class will manage the overall simulation workflow.
*   **A `SimulationRunner` Component:** This will be a wrapper around the chosen simulation code (e.g., LAMMPS). It will be responsible for preparing the input scripts for the simulation, launching the external code, and monitoring its execution.
*   **An `UncertaintyQuantifier` Component:** This is the core innovation of Cycle 4. It will be tightly coupled with the `SimulationRunner`. It needs a way to get the current atomic configuration from the running simulation at each step. It will then calculate an uncertainty score. The initial implementation might use a simple, fast-to-compute metric, such as the disagreement between a committee of models or the magnitude of the latent representation in a GNN model.
*   **A `StructureExtractor` Component:** This component is triggered when the `UncertaintyQuantifier` flags a structure. It will be responsible for correctly extracting the configuration from the simulation and preparing it for the labeling engine, including the periodic embedding logic.

This design separates the concerns of running the simulation from the logic of quantifying uncertainty, making each component easier to develop and test.

## 3. Design Architecture

The design for Cycle 4 will involve creating the new `SimulationEngine` module and significantly enhancing the main orchestrator to manage the new cyclical workflow.

**Updated Project Structure:**
```
src/mlip_autoflow/
├── __init__.py
├── main.py                # Will now contain the main active learning loop logic
├── config/
│   └── models.py
└── modules/
    ├── a_structure_generator.py
    ├── b_explorer_sampler.py
    ├── c_labeling_engine.py
    ├── d_training_engine.py
    └── e_simulation_engine.py   # New file
```

**Class and Method Definitions:**

*   **`config.models.py`**: The `FullConfig` Pydantic model will be extended.
    *   A new `SimulationConfig` model will be added. This will include parameters for the production simulation, such as `md_ensemble` (NVE, NVT), `temperature`, `timestep`, `total_simulation_time`.
    *   A new `ActiveLearningConfig` model will define parameters for the OTF loop, such as `uncertainty_threshold`, `max_active_learning_cycles`.

*   **`modules.e_simulation_engine.py`**: This file will house the `SimulationEngine` class.
    *   `__init__(self, sim_config: SimulationConfig, al_config: ActiveLearningConfig)`: Initializes with its configuration.
    *   `run_otf_simulation(self, initial_model_path: str, initial_structure: ase.Atoms)`: The main public method. This method will contain the loop that runs the simulation step-by-step or in chunks.
    *   `_launch_simulation(self, model_path, structure)`: A private method to start or restart the simulation subprocess (e.g., LAMMPS).
    *   `_check_uncertainty(self, current_structure) -> bool`: A private method that calls the UQ logic and returns `True` if the threshold is exceeded. The initial UQ method could be based on the local atomic environment of each atom, comparing it to the environments in the training set.
    *   `_extract_structure_for_retraining(self) -> ase.Atoms`: A private method that extracts the current structure when the uncertainty is high. It will implement the periodic embedding logic.

*   **`main.py`**: The main orchestrator script will be significantly refactored. The linear workflow will be wrapped inside a new `for` loop that iterates for a maximum number of active learning cycles.
    *   **Outer Loop:** `for cycle in range(max_active_learning_cycles):`
    *   **Initial Run (cycle 0):** The existing workflow (A -> B -> C -> D) is run to get the first MLIP.
    *   **Simulation Step:** The `SimulationEngine` is instantiated and its `run_otf_simulation` method is called. This method will run until it either completes or returns a high-uncertainty structure.
    *   **Check for Exit:** If the simulation completes successfully, the loop breaks.
    *   **Re-training Step:** If a new structure is returned, the orchestrator adds it to the dataset, and calls `Module C` and `Module D` again to produce a new, improved MLIP. The path to this new model is passed back to the `SimulationEngine`, and the outer loop continues to the next iteration.

This refactored orchestrator design turns our pipeline into a stateful process, where the state (the training dataset and the MLIP) is progressively refined in each iteration of the active learning loop.

## 4. Implementation Approach

The implementation of Cycle 4 requires integrating with an external simulation code and building the logic for the feedback loop.

**Step 1: Select and Integrate a Simulation Engine**
The first decision is which simulation code to use. LAMMPS is a strong candidate due to its wide support for various MLIPs. The first implementation step will be to create a simple wrapper in the `SimulationEngine` that can:
1.  Take a model file and an `ase.Atoms` object.
2.  Generate a basic LAMMPS input script for a simple MD run.
3.  Launch LAMMPS as a subprocess.
This will require adding logic to `pyproject.toml` to ensure LAMMPS is installed or built as part of the project setup.

**Step 2: Implement Uncertainty Quantification (UQ)**
This is the most research-intensive part of the cycle. We will start with a simple and pragmatic UQ method. One possible approach is a novelty-based method:
1.  During the initial training (`Module D`), pre-calculate and store the descriptors (e.g., SOAP vectors) for all atoms in the training set.
2.  In `_check_uncertainty`, calculate the descriptor for the current atom in the simulation.
3.  Find the "closest" descriptor in the training set (e.g., by cosine similarity).
4.  If the distance to the closest training example is above a certain threshold, the atom is in a novel environment, and the uncertainty is high.
This method is simple to implement and requires minimal changes to the training engine. We will implement this logic inside the `SimulationEngine`.

**Step 3: Develop the OTF Execution Loop**
We will implement the main `run_otf_simulation` method. A key challenge is running the simulation and the UQ check concurrently. A simple approach is to run the simulation in short chunks. For example:
1.  Run the LAMMPS simulation for 100 steps.
2.  Pause LAMMPS.
3.  Read the last frame from the trajectory.
4.  Run the `_check_uncertainty` method on that frame.
5.  If uncertainty is low, loop and run the next 100 steps.
6.  If uncertainty is high, extract the structure and return it, breaking the loop.
This chunk-based execution is easier to implement than a true step-by-step integration.

**Step 4: Implement Periodic Embedding Extraction**
When a high-uncertainty atom is identified, we need to extract its local environment. We will implement the `_extract_structure_for_retraining` method. It will take the full simulation cell, identify the high-uncertainty atom, and then cut out a smaller, periodic cell centered on that atom. This ensures that the DFT re-calculation is performed on a small, manageable system that still preserves the essential periodic nature of the local environment.

**Step 5: Refactor the Main Orchestrator**
The final and most significant step is to refactor `main.py` to implement the active learning `for` loop as described in the Design Architecture. This will involve managing the state—the growing dataset and the path to the latest MLIP—across iterations. We will add extensive logging to track the progress of the active learning cycles, recording when uncertainties are triggered and a new generation of the MLIP is created.

## 5. Test Strategy

Testing Cycle 4 is challenging due to the introduction of a stateful, cyclical workflow and the reliance on an external simulation program.

**Unit Testing Approach:**
*   **`SimulationEngine`**: We will heavily mock the external LAMMPS process.
    *   **UQ Test:** We will create a dedicated test for the `_check_uncertainty` method. We will create a dummy training set of descriptors and then provide a new descriptor that is intentionally "far" from the training set, asserting that the method correctly returns `True`.
    *   **Extraction Test:** We will test the `_extract_structure_for_retraining` method. We will provide a large `ase.Atoms` object and assert that the method correctly extracts a smaller periodic cell of the expected size and composition centered on the correct atom.
    *   **Runner Test:** We will test the logic that generates the LAMMPS input script, asserting that for a given configuration, the correct commands (e.g., for ensemble, temperature) are written to the script.

**Integration Testing Approach:**
The integration test for Cycle 4 is essentially a "mini" end-to-end test of the entire active learning loop.
*   **Test Scenario:** The test will be designed to deterministically trigger the uncertainty mechanism at least once.
    1.  **Setup:**
        *   Create a very small initial training set for a simple system (e.g., Si), but intentionally leave out a specific type of configuration (e.g., a highly compressed structure).
        *   Configure the active learning parameters for a maximum of 2-3 cycles and set the uncertainty threshold to a sensitive value.
    2.  **Execution:**
        *   Run the initial training (Modules C and D) to produce a "bad" initial MLIP that is known to be poor for compressed structures.
        *   Start the `SimulationEngine`. The simulation protocol will be designed to intentionally steer the system into the configuration that was omitted from the training set (e.g., by applying high pressure).
        *   The system should run, detect high uncertainty when it enters the compressed state, pause, and extract the structure.
        *   The orchestrator should then label this new structure and re-train the model.
        *   The simulation should then resume with the new, improved model.
    3.  **Verification:**
        *   Check the logs to confirm that the uncertainty mechanism was triggered.
        *   Verify that the database now contains the new, compressed structure.
        *   Confirm that a "generation 1" MLIP file was created.
        *   Assert that the simulation, when resumed, runs more stably with the new potential.

This test provides strong evidence that the entire feedback loop is working as designed: the system can detect its own failures, gather the necessary data to correct them, and automatically improve itself. This is the core validation for the deliverables of Cycle 4.
