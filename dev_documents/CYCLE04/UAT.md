# Cycle 4 User Acceptance Tests: Active Learning and Advanced Simulation

**Version:** 1.0.0
**Status:** Final

## 1. Test Scenarios

This document outlines the User Acceptance Tests (UAT) for Cycle 4. These tests are designed to validate the project's most advanced and powerful feature: the on-the-fly (OTF) active learning loop, implemented in **Module E (Simulation Engine)**. From a user's perspective, this cycle is about trust and autonomy. The tests will verify that the system can be trusted to run a long simulation, intelligently identify its own weaknesses, and autonomously perform the necessary steps to correct them, resulting in a more robust and accurate final model. We are no longer just testing a data processing tool, but a semi-sentient scientific instrument that actively improves itself. The scenarios are therefore focused on the behavior of this loop and the quality of its outcomes.

| Scenario ID | Description | Priority |
| :--- | :--- | :--- |
| **UAT-C4-001** | **Successful Active Learning Loop Execution:** This is the cornerstone test for this cycle. It validates the entire, complex feedback loop in a real-world scientific scenario (a phase transition). The user needs to see clear, unambiguous evidence that the system detected a problem (high uncertainty), paused the simulation, and correctly executed the entire retraining workflow (labelling, database update, retraining, resuming). Its success is the primary proof that the core feature of this cycle is functional. | **High** |
| **UAT-C4-002** | **Configurability of the OTF Loop:** An autonomous system that cannot be controlled is a liability. This test ensures the user has meaningful control over the active learning loop's behavior. By testing different `uncertainty_threshold` values, the user can verify that they can tune the trade-off between computational cost and model quality. A high threshold should lead to a fast, cheap run that only fixes major errors, while a low threshold should lead to a more expensive, higher-quality run. This confirms the user is still in command of the scientific direction and budget. | **High** |
| **UAT-C4-003** | **Measurable Improvement in Potential Quality:** The active learning loop is complex and computationally expensive. This test provides the crucial validation that the effort is worthwhile. By directly comparing the performance of the initial model with the final, retrained model on a structure that was discovered during the simulation, the user can see concrete, quantitative proof that the system is not just running, but learning. This builds trust that the final artifact is genuinely superior to the one generated by the static pipeline. | **High** |
| **UAT-C4-004** | **Graceful Completion of Simulation:** A user needs to be confident that a long-running simulation will eventually complete. An active learning loop carries the risk of getting stuck in an infinite cycle of retraining. This test verifies the safeguards that prevent this. By confirming that a long simulation either converges to a stable state (no more triggers) or exits gracefully upon hitting a user-defined limit on the number of retraining cycles, this test assures the user that their jobs will be deterministic and will not run indefinitely. | **Medium** |
| **UAT-C4-005** | **Advanced Simulation (kMC) Run:** This scenario validates the scientific breadth of the new `SimulationEngine`. It ensures that the module is not just an MD engine but a more general platform for atomistic simulations. By running an Adaptive Kinetic Monte Carlo simulation, the user can verify the system's ability to study a different class of physical phenomena—rare events—which are inaccessible to standard MD. This confirms the value of the platform for a wider range of scientific problems, such as catalysis, diffusion, and long-term degradation. | **Medium** |

---

## 2. Behavior Definitions

### **UAT-C4-001: Successful Active Learning Loop Execution**

**Scenario:** A researcher is studying the melting of aluminum. They have generated an initial MLIP using a dataset that was primarily composed of the solid, crystalline phase at low temperatures. They now want to run a simulation that heats the aluminum past its melting point, and they need the system to automatically learn about the liquid phase as it emerges.
> **GIVEN** an initial MLIP for aluminum (`al_initial.model`) that is known to be accurate for the solid phase but has not been trained on any liquid configurations.
> **AND** a configuration file that specifies a molecular dynamics simulation to be run at a high temperature (e.g., 1200 K), well above aluminum's experimental melting point.
> **AND** the `active_learning` section in the configuration is enabled, with a moderate `uncertainty_threshold`.
>
> **WHEN** the user starts the simulation.
>
> **THEN** the system should begin the MD run, and the initial log messages should show the system's temperature stabilizing around the target 1200 K.
> **AND** as the simulated crystal structure starts to disorder and lose its long-range order, the log output must show a clear message indicating that a high-uncertainty event has been triggered by an atom in a novel, liquid-like environment.
> **AND** the log must immediately indicate that the simulation is being gracefully paused, saving its current state.
> **AND** it must then log that a new structure is being extracted and sent to the DFT labelling engine for calculation.
> **AND** after the DFT calculation is finished, the log must show that the training engine is being invoked to create an updated MLIP.
> **AND** finally, the log must clearly indicate that the simulation has resumed with the new, improved potential, continuing from the exact point it was paused.

### **UAT-C4-002: Configurability of the OTF Loop**

**Scenario:** A user is exploring a new, complex ternary alloy. They want to perform two different runs: first, a quick, "scouting" run that only retrains on the most egregiously uncertain events to get a rough idea of the potential's quality. Second, a high-quality "production" run that is very sensitive to any hint of uncertainty to generate a publication-quality potential.
> **GIVEN** an identical initial MLIP and simulation setup for the ternary alloy.
>
> **WHEN** the user performs the first run with a high, permissive `uncertainty_threshold` (e.g., `10.0`) in their configuration file.
> **AND** after it completes, the user performs a second run on the exact same system, but this time with a low, strict `uncertainty_threshold` (e.g., `2.0`).
>
> **THEN** the console output and logs from the first run should show that the retraining loop was triggered only a few times (e.g., "Retraining cycle 1/5", "Retraining cycle 2/5").
> **AND** the console output and logs from the second run should show that the retraining loop was triggered a significantly greater number of times (e.g., "Retraining cycle 1/20", "Retraining cycle 2/20", ... "Retraining cycle 15/20").
> **AND** inspecting the final ASE databases from both runs should show that the second run's database contains significantly more new structures added by the active learning loop.
> **AND** both runs should complete successfully, demonstrating the user's direct and effective control over the thoroughness and cost of the process.

### **UAT-C4-003: Measurable Improvement in Potential Quality**

**Scenario:** A user wants to be absolutely certain that the computationally expensive active learning process is providing a tangible benefit. They need to see quantitative proof that the final, retrained model is superior to the initial one.
> **GIVEN** an active learning simulation has completed, producing two model files in the output directory: `initial.model` (the model before the OTF loop) and `final.model` (the model after all retraining cycles).
> **AND** the ASE database from the run shows that a specific, high-energy, distorted structure (with `id=50`, for example) was added to the training set during the active learning process. The DFT-calculated energy for this structure is known.
>
> **WHEN** the user runs a separate, provided validation script that takes both model files and the specific structure (`id=50`) as input.
>
> **THEN** the script's output for the `initial.model` should show a large and unacceptable deviation from the true DFT energy for structure `id=50`. For example, it might predict an energy that is off by more than 100 meV/atom.
> **AND** the script's output for the `final.model` should show a much, much smaller deviation for the same structure. For example, its predicted energy should now be within 5 meV/atom of the DFT value.
> **AND** this direct comparison provides concrete, quantitative evidence that the potential has learned from the new data and has significantly improved its accuracy in a previously unknown domain.

### **UAT-C4-004: Graceful Completion of Simulation**

**Scenario:** A user sets up a very long simulation, intended to run for 100,000 MD steps on a high-performance computing cluster. They need to be confident that the job will finish and not get stuck in an endless loop of finding and retraining on minor uncertainties.
> **GIVEN** a simulation is configured to run for a total of 100,000 MD steps with the active learning loop enabled.
> **AND** the configuration also includes a safeguard parameter, `max_retrain_cycles`, which is set to a reasonable number like 20.
>
> **WHEN** the user submits this long-running simulation.
>
> **THEN** the system may trigger several retraining events in the initial, "burn-in" phase of the simulation, and the log should show the retraining cycle counter decreasing (e.g., "19 retraining cycles remaining...").
> **AND** as the potential becomes more robust for the simulated conditions, the log should show that the simulation is running for longer and longer periods (thousands of steps) between uncertainty triggers.
> **AND** the simulation must ultimately complete all 100,000 steps successfully and exit gracefully with a zero exit code. This can happen in one of two valid ways: either the potential becomes so robust that no more triggers occur, or the system hits the `max_retrain_cycles` limit and continues the rest of the simulation with the final potential, logging a warning that the limit was reached.

### **UAT-C4-005: Advanced Simulation (kMC) Run**

**Scenario:** A researcher is studying lithium ion diffusion in a solid-state electrolyte. This process is dominated by rare atomic hopping events that happen on a timescale of nanoseconds or longer, making it impossible to simulate with standard MD. They want to use the system's new Adaptive Kinetic Monte Carlo capability.
> **GIVEN** a high-quality, robust MLIP has been generated for the lithium electrolyte material, which contains a single lithium vacancy.
> **AND** the simulation in the configuration file is set to run in the `adaptive_kmc` mode, with a target simulation time of several nanoseconds.
>
> **WHEN** the user starts the simulation.
>
> **THEN** the log output should clearly indicate that it is starting an "Adaptive Kinetic Monte Carlo" run, not an MD run.
> **AND** the log should show that the system is performing saddle point searches (e.g., using the NEB or Dimer methods) to find the energy barriers for lithium atoms hopping into the nearby vacancy.
> **AND** the log should show the kMC clock advancing, and the time steps should be much larger than typical MD femtosecond steps (e.g., showing jumps of picoseconds or nanoseconds after each event).
> **AND** the final output of the run should include a summary of the discovered diffusion events, their calculated energy barriers, and the effective diffusion coefficient of lithium in the material.
