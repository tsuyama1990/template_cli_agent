# Cycle 05 User Acceptance Test (UAT): UI/UX, Finalisation, and Documentation

## 1. Test Scenarios

This document outlines the User Acceptance Tests for Cycle 05. The focus of these tests is on the overall user experience (UX) and the quality and polish of the final software package. The scenarios are designed to verify that the command-line interface is intuitive and robust, that the application provides adequate and clear feedback during its long execution times, and that the documentation is clear, correct, and genuinely helpful to a new user. These tests are less about the scientific correctness of the underlying modules (which were tested in previous cycles) and more about the holistic experience of using the tool.

| Scenario ID | Description                                                              | Priority |
|-------------|--------------------------------------------------------------------------|----------|
| UAT-05-001  | **Successful Execution via CLI:** Verify that a user can successfully launch and run the entire pipeline from start to finish using a single, simple command from their terminal. This test ensures the main entry point is working correctly and that the application is properly packaged. | High     |
| UAT-05-002  | **Clear Progress Reporting:** Verify that the application provides clear visual feedback on its progress during long-running tasks, ensuring the user understands what the system is doing and is not left staring at a blank or uninformative screen. | High     |
| UAT-05-003  | **Verbose Logging Control:** Verify that the user can easily control the level of detail in the log output using a command-line flag, allowing them to access detailed debugging information when necessary without being overwhelmed by it during normal operation. | High     |
| UAT-05-004  | **Helpful Error Message on Bad CLI Usage:** Verify that the CLI provides helpful, human-readable guidance if the user provides incorrect commands, options, or arguments. This is a key test of the tool's user-friendliness in the face of common errors. | Medium   |
| UAT-05-005  | **Tutorial is Accurate and Reproducible:** Verify that a new user, starting from scratch, can follow the "Getting Started" tutorial in the documentation and successfully run the provided example case, obtaining the expected results. This is the ultimate test of the quality and accuracy of the user documentation. | High     |

## 2. Behaviour Definitions

The following Gherkin-style definitions describe the expected behaviour for each test scenario in detail.

---

### **Scenario: UAT-05-001 - Successful Execution via CLI**

This scenario tests the primary user interaction with the finished application. The goal is to ensure that the complexity of the internal orchestrator and the various modules is completely hidden behind a simple, clean command-line interface. The user should be able to initiate a complex scientific workflow with a single, memorable command.

**GIVEN** a user has prepared a valid `input.yaml` file for a simple system.
**AND** the user has opened a terminal and activated the application's Python virtual environment where the `mlip-pipe` command is available.
**WHEN** the user types the command `mlip-pipe run --input-file input.yaml` and presses Enter.
**THEN** the MLIP-AutoPipe workflow should start and execute successfully without requiring any further input from the user.
**AND** the final output generated by the run (including log files, the ASE database, and the trained model file) should be identical to the output that would be generated by running the pipeline via a direct Python script.
**AND** upon successful completion of the entire workflow, the command should exit gracefully with a status code of 0, indicating success to the shell.

---

### **Scenario: UAT-05-002 - Clear Progress Reporting**

This scenario tests the user experience during a long run. A complex workflow like MLIP-AutoPipe can take several hours or even days to complete. It is unacceptable for the user to be left without feedback during this time. This test ensures that the system provides continuous, informative feedback on its status.

**GIVEN** a pipeline execution that is configured to run for a significant amount of time (e.g., several active learning generations).
**WHEN** the user launches this pipeline via the `mlip-pipe run` command.
**THEN** the console should display a high-level progress bar for the overall workflow, for instance showing "Active Learning: [=====>----] 3/10 Generations".
**AND** within each generation, for time-consuming tasks like DFT labelling or MD simulation, a nested progress bar should be displayed. For example, during labelling, it should show "Labelling Structures: [========>--] 40/50".
**AND** the progress bar should show a realistic percentage of completion and, if possible, an estimated time remaining.
**AND** in addition to the progress bars, the console should print clear, high-level status messages as the workflow transitions between major stages, such as "Configuration successfully expanded.", "Starting structure generation...", "50 candidate structures selected. Starting DFT labelling...", or "Training new MLIP for generation 3...".

---

### **Scenario: UAT-05-003 - Verbose Logging Control**

This scenario tests the user's ability to control the information they receive from the application. By default, the output should be clean and concise, but the user must have a simple way to access more detailed information for debugging or for a deeper understanding of the process.

**GIVEN** a standard execution of the pipeline using the command `mlip-pipe run --input-file input.yaml`.
**THEN** the console output should be clean, showing only high-level `INFO` messages, warnings, errors, and the progress bars.
**AND** a log file named `mlip-autopipec.log` should be created in the output directory, containing these same `INFO`, `WARNING`, and `ERROR` messages with timestamps.
---
**GIVEN** the same `input.yaml` file.
**WHEN** the user executes the pipeline with the verbose flag: `mlip-pipe run --input-file input.yaml --verbose`.
**THEN** the console output should become much more detailed, including `DEBUG` level messages. These messages might include, for example, the specific parameters being written to a Quantum Espresso input file, the internal state of the sampling algorithm, or the loss values during the MLIP training.
**AND** the `mlip-autopipec.log` file for this run should also contain these detailed `DEBUG` messages, providing a complete record for post-mortem analysis.

---

### **Scenario: UAT-05-04 - Helpful Error Message on Bad CLI Usage**

This scenario tests how the application handles incorrect usage of its primary interface, the CLI. A user-friendly tool should not crash with a long, cryptic traceback but should instead provide helpful, actionable feedback.

**GIVEN** a user is in their terminal with the application's environment activated.
**WHEN** the user types `mlip-pipe run` without specifying the mandatory input file option.
**THEN** the application should not start the pipeline but should immediately exit with a non-zero status code.
**AND** it should print a clear and helpful error message to the console, such as "Error: Missing option '--input-file'. This option is required."
**AND** it should also suggest a helpful next step, such as "Try 'mlip-pipe run --help' for more information."
---
**WHEN** the user types `mlip-pipe --help` or `mlip-pipe run --help`.
**THEN** the console should display a well-formatted and easy-to-read help message. This message should list all available commands (e.g., `run`), all the options for that command (e.g., `--input-file`, `--verbose`), and a clear, concise description of what each command and option does.

---

### **Scenario: UAT-05-005 - Tutorial is Accurate and Reproducible**

This scenario is the ultimate test of the user documentation. Documentation is only useful if it is correct and leads to a successful outcome. This test ensures that a brand-new user can successfully get the software running by following the provided instructions.

**GIVEN** a user has a fresh installation of the MLIP-AutoPipe software on a new machine.
**AND** this user has no prior experience with the tool.
**WHEN** the user opens the `docs/getting_started.md` file from the project's repository.
**AND** they follow the instructions for setting up the Python environment and installing the dependencies.
**AND** they copy and paste the example `input.yaml` from the tutorial into a new file.
**AND** they run the exact command specified in the tutorial to launch the pipeline.
**THEN** the pipeline should execute successfully from start to finish without any errors.
**AND** the final output files generated by the run (e.g., the `model.ace` file and the `exec_config_dump.yaml`) should match the description of the expected output in the tutorial document.
**AND** the user should be able to successfully complete the tutorial without needing any information not present in the document itself.