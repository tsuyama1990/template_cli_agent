# System Architecture: MLIP-AutoPipe

## 1. Summary

The MLIP-AutoPipe project is an advanced, automated framework designed to generate high-quality, physically valid, and diverse training data for Machine Learning Interatomic Potentials (MLIPs), such as MACE and SevenNet. The core philosophy of this tool is to remove the human expert from the loop, replacing manual, often intuition-driven data generation with a robust, reproducible, and scientifically grounded pipeline. The development of accurate MLIPs is a critical bottleneck in computational materials science. While these models promise the accuracy of quantum mechanical calculations at a fraction of the computational cost, their predictive power is entirely dependent on the quality and diversity of the data they are trained on. Traditional methods of generating this data are often laborious, requiring significant domain expertise to create structures that are not only stable but also adequately explore the potential energy surface (PES). An MLIP trained only on low-energy, equilibrium structures will perform poorly when simulating high-temperature dynamics, phase transitions, or defect formation—precisely the scientifically interesting phenomena researchers wish to study. This system directly addresses this challenge by integrating established simulation techniques—Molecular Dynamics (MD) and Monte Carlo (MC)—into a fully automated workflow designed to intelligently probe the PES.

The primary objective is to sample the vast configurational phase space of a given material system in a physically meaningful way. Instead of merely generating random atomic arrangements, which are often unphysical or thermodynamically irrelevant, MLIP-AutoPipe employs simulation engines to mimic real physical processes. By simulating at high temperatures or applying volumetric strain, the system encourages atoms to explore high-energy configurations, saddle points, and transition states. These "hard" or "confusing" examples are precisely what MLIPs need to learn the underlying physics accurately and generalize to unseen configurations. This approach can be seen as a form of implicit active learning, where the MLIP itself is used to explore regions of the PES where it is likely to be uncertain. The framework is designed from the ground up for versatility, capable of handling a wide variety of material systems, from simple binary alloys and ionic crystals to complex interfaces and surfaces with adsorbed molecules. This versatility is achieved through a modular, factory-based approach where specific "Generator" classes are responsible for creating initial "seed" structures based on user-defined parameters like chemical composition, crystal symmetry, or known database entries. The entire pipeline is architected as a sequence of distinct, state-isolated stages: Generation, Exploration, Sampling, and Storage. This separation of concerns is a key design principle, ensuring that each step is independently verifiable and robust. For instance, generated structures are immediately saved to a checkpoint before the computationally intensive exploration phase begins, preventing data loss in case of a system crash. The final output is not just a collection of raw atomic coordinates, but a curated, well-structured ASE database containing not only the structures but also all associated metadata (energy, forces, stress), ready for direct ingestion into MLIP training frameworks like Allegro or MACE. This focus on an end-to-end, reproducible, and extensible workflow is what elevates MLIP-AutoPipe from a simple script to a powerful research tool.

## 2. System Design Objectives

The design of MLIP-AutoPipe is guided by a set of clear objectives aimed at creating a tool that is powerful, flexible, and reliable for researchers in computational materials science. These objectives translate into specific design choices and success criteria for the project.

**Primary Objectives:**
*   **Full Automation:** The foremost goal is to fully automate the end-to-end process of training data generation. In practice, this means minimizing human intervention to a single configuration file and a single command. The system must autonomously handle file I/O, subprocess management, error detection, and data aggregation. The ideal user experience involves the user defining their scientific problem in the input YAML, executing the tool, and returning later to find a complete, ready-to-use database. This objective directly influences the need for a robust `WorkflowOrchestrator` that manages the entire lifecycle without requiring interactive input. It also implies a sophisticated error-handling mechanism that can gracefully catch and log failures in individual simulations without halting the entire batch.

*   **Guaranteed Physical Validity:** Every atomic structure processed and ultimately stored by the pipeline must be physically plausible. This is a hard constraint, as unphysical structures (e.g., with overlapping atoms) can introduce noise and instability into the MLIP training process. This objective is enforced through a multi-layered validation strategy. At the generation stage, an `overlap_check` function, which uses covalent radii to ensure atoms are not unnaturally close, is mandatory. For ionic systems, generators must include logic to enforce local and global charge neutrality. During exploration, the choice of the correct thermodynamic ensemble is critical; the `auto_ensemble` feature, which selects NPT for bulk systems to allow cell relaxation and NVT for surfaces to prevent vacuum layer collapse, is a direct implementation of this objective. This prevents common simulation artefacts that could corrupt the dataset.

*   **Maximum Diversity and Richness:** The generated dataset must be diverse enough to allow the MLIP to generalize well. Diversity has two components: energetic and structural. Energetic diversity is achieved through the exploration engine, which pushes the system to high-energy states via high-temperature MD. However, many high-energy states can be structurally similar. Therefore, structural diversity is achieved via the Farthest Point Sampling (FPS) algorithm. This objective dictates the inclusion of FPS as a core feature. Unlike random sampling, which may oversample long-lived, stable states, FPS explicitly prioritizes configurations that are structurally unique, as measured by their distance in a high-dimensional feature space defined by SOAP descriptors. The synergy between high-T MD and FPS is central to achieving a dataset that is rich in both physically challenging and structurally unique configurations.

*   **Modularity and Extensibility:** The architecture must be modular to facilitate future expansion, as the field of materials science is constantly evolving. This objective is realized through a strict adherence to object-oriented design principles, particularly the use of abstract base classes and factory patterns. For example, to add a new `MolecularCrystalGenerator`, a developer would only need to create a new class inheriting from `BaseGenerator` and implement its `generate` method. They would then register this new generator in the `GeneratorFactory`. No changes would be needed in the core `WorkflowOrchestrator`. This design pattern applies to all key components: samplers, labeling engines, and explorers, ensuring that the tool can be easily adapted to new scientific challenges or integrated with new simulation codes without requiring a major rewrite.

**Constraints and Success Criteria:**
*   **Performance and Scalability:** The system must be performant enough to handle simulations of systems containing hundreds to thousands of atoms and to scale across multiple CPU cores. The primary design choice to meet this constraint is the use of Python's `ProcessPoolExecutor` to parallelize the independent MD simulations. A critical performance consideration is the memory footprint of the MLIP models themselves. A naive implementation might pickle the loaded MLIP model and send it to each worker process, which is inefficient and often fails for complex PyTorch models. To overcome this, the system employs a "late-binding" calculator pattern, where each worker process is responsible for loading its own instance of the MLIP model, avoiding inter-process communication overhead.

*   **Robustness and Fault Tolerance:** Long-running scientific workflows are prone to failure. The exploration phase is particularly vulnerable. The system must be resilient. This is achieved by isolating individual simulations within the parallel runner. If one simulation crashes due to a "Coulomb explosion" or another `PhysicsViolationError`, it is caught, logged, and the structure is saved to a "failed" database for later inspection. This prevents a single unstable configuration from crashing the entire workflow. Furthermore, trajectory data is saved progressively to disk, so that valuable computational effort is not lost if the application is terminated unexpectedly.

*   **Excellent User Experience:** The tool must be accessible to its target audience. This objective leads to the requirement of a dual interface. The Command Line Interface (CLI), built with `typer`, provides power and scriptability for automated workflows and integration into high-performance computing (HPC) schedulers. The secondary Web UI, built with a simple framework like Streamlit, provides an interactive, user-friendly way for researchers to define their system, configure a workflow, and visualize the results, lowering the barrier to entry.

The ultimate success of the project will be measured by its ability to generate a training dataset that, when used to train a standard MLIP model, results in a potential with demonstrably lower prediction errors for energy, forces, and stress compared to a dataset generated by simpler means. A key benchmark will be the ability of the resulting potential to run stable, long-time MD simulations of the target material system at various temperatures.

## 3. System Architecture

The MLIP-AutoPipe system is architected as a modular, four-stage sequential pipeline orchestrated by a central `WorkflowOrchestrator`. Each stage is a self-contained component with a well-defined responsibility and a clear data contract (input/output), ensuring a robust and comprehensible flow of data. This design promotes separation of concerns and facilitates independent development and testing of each component.

```mermaid
graph TD
    subgraph User Input
        A[Config File .yaml]
    end

    subgraph "Stage 1: Generation"
        B(Structure Generators)
        B -->|List[ase.Atoms]| C{Validation & Pre-processing}
    end

    subgraph "Stage 2: Exploration"
        D(Hybrid MD/MC Engine)
        D -->|List[Trajectories]| E[Parallel Runner]
    end

    subgraph "Stage 3: Sampling"
        F(Sampling Module)
        F -->|List[ase.Atoms]| G[FPS / Random]
    end

    subgraph "Stage 4: Storage"
        H(Database Storage)
        H -->|Final Dataset| I[ASE Database .db]
    end

    A -- Pydantic Loads --> B
    C -- Passes In-Memory --> D
    E -- Passes In-Memory --> F
    G -- Passes In-Memory --> H
```

**Component Breakdown:**
1.  **Configuration (`Config File .yaml`):** This is the sole entry point for user interaction. The entire workflow is defined declaratively in a single YAML file. The choice of a Pydantic-based configuration loading system is deliberate. It provides several key advantages over manual parsing. First, it offers automatic validation at runtime; if a user provides a negative number for simulation steps or misspells a parameter, the application will fail immediately with a clear, informative error message, preventing wasted computation. Second, it makes the code self-documenting, as the Pydantic models in `config.py` serve as the definitive schema for the configuration file. This validated configuration object is then passed to the orchestrator and serves as the single source of truth for the entire workflow.

2.  **Generation Stage (`Structure Generators`):** This stage is responsible for creating the initial "seed" structures. Its input is the `SystemConfig` and `GenerationConfig` sections of the main configuration object. A `GeneratorFactory` acts as a dispatcher, reading the configuration to determine which specific generator to instantiate (e.g., `AlloyGenerator`, `IonicGenerator`). These generators are specialized classes that encapsulate the logic for building different types of material systems, using foundational libraries like `ASE` and `pymatgen`. A crucial part of this stage is the mandatory validation and pre-processing step. Every generated structure, regardless of its source, is passed through a gauntlet of checks (e.g., `overlap_check`) and augmentations (e.g., applying volumetric strain or atomic rattling). The output of this stage is a well-defined list of `ase.Atoms` objects, ready for exploration.

3.  **Exploration Stage (`Hybrid MD/MC Engine`):** This is the computational core of the system. It receives the list of seed structures from the generation stage. The `Parallel Runner` component distributes these structures among a pool of worker processes. Inside each worker, the `MDMCExplorer` takes over. It first performs the "late-binding" of the MLIP calculator, loading the model into memory within the worker to avoid serialization issues. It then runs a simulation. In the case of a hybrid MD/MC run, the engine integrates custom logic into the standard MD loop. At a specified frequency, the MD is paused, and a Monte Carlo move (e.g., swapping the positions of two different atomic species) is attempted. The potential energy of the proposed new configuration is evaluated using the MLIP, and the move is accepted or rejected based on a Metropolis acceptance criterion. This allows for efficient exploration of both configurational and compositional space simultaneously. The output of this stage is a list of trajectories, where each trajectory is a list of `ase.Atoms` objects representing the system's state at different points in time.

4.  **Sampling Stage (`Sampling Module`):** The exploration stage can produce an enormous amount of data—potentially tens of thousands of structures. The sampling module's responsibility is to intelligently select a small, information-rich subset from this data deluge. Its input is the list of trajectories from the previous stage. A `SamplerFactory` uses the `SamplingConfig` to determine which strategy to employ. For Farthest Point Sampling (FPS), the module first computes SOAP descriptors for every structure in the trajectories. It then uses an iterative algorithm to select a subset of structures that are maximally distant from each other in this high-dimensional descriptor space. The output of this stage is a single, flattened list of `ase.Atoms` objects, representing the final, curated dataset.

5.  **Storage Stage (`Database Storage`):** The final stage takes the curated list of structures from the sampling stage. For each structure, it calls the `LabelingEngine` to compute the final energy, forces, and stress (this could be a real DFT calculation or simply retrieving the values from the MLIP used in exploration). It then uses the `AseDBWrapper` to save each structure along with its computed properties into an ASE Database. This database is a standard, portable SQLite file, which serves as the final, deliverable artifact of the pipeline.

## 4. Design Architecture

The project's software architecture is designed to be modern, robust, and maintainable, adhering to best practices in Python software engineering. The core principles are schema-first development, clear separation of concerns, and dependency injection.

**File Structure and Rationale:**
The file structure is intentionally layered to decouple different aspects of the application.

```
.
├── pyproject.toml
├── src
│   └── mlip_autopipec
│       ├── __init__.py
│       ├── cli.py              # UI Layer (Typer)
│       ├── config.py           # Schema Layer (User-facing Pydantic models)
│       ├── main.py             # Application entrypoint
│       ├── common
│       │   ├── __init__.py
│       │   ├── pydantic_models.py # Domain Layer (Internal Pydantic models)
│       │   └── exceptions.py     # Custom exceptions
│       ├── core
│       │   ├── __init__.py
│       │   ├── ase_db_wrapper.py # Infrastructure Layer
│       │   ├── generators.py     # Service/Domain Logic Layer
│       │   ├── labeling.py       # Service/Domain Logic Layer
│       │   └── workflow.py       # Service/Orchestration Layer
│       └── utils
│           └── ...
└── tests
    └── ...
```

-   **`cli.py` (UI Layer):** This layer is responsible only for user interaction. It contains no core business logic. Its job is to parse command-line arguments, load the configuration file, and delegate control to the orchestration layer (`WorkflowOrchestrator`). This separation means the core logic can be reused with a different UI (like a web interface) without modification.
-   **`config.py` & `common/pydantic_models.py` (Schema/Domain Layer):** This is the heart of the schema-first design. `config.py` defines the "public API" of the application—the structure of the YAML file that users will write. `common/pydantic_models.py` defines the internal data structures that are passed between different components of the pipeline, such as `DFTResult`. These models act as explicit, validated contracts between different parts of the code.
-   **`core/` (Service & Infrastructure Layers):** This directory contains the application's "engine". The `workflow.py` file orchestrates the high-level process but delegates the actual work to specialized modules like `generators.py` (which knows how to build atoms), `labeling.py` (which knows how to run DFT codes), and `ase_db_wrapper.py` (which knows how to talk to the database). This promotes high cohesion (each module does one thing well) and low coupling (modules interact through well-defined interfaces and data models, not by reaching into each other's internal state).

**Schema-First Development Deep Dive:**
The entire development process is anchored by the principle of "Schema-First". Before any procedural code is written, we first define the data.
1.  **Define `FullConfig`:** The process begins by defining the master `FullConfig` Pydantic model. This is a design exercise that forces us to think about every single parameter the user will need to control. We define not just the parameter names and types, but also their constraints (e.g., `num_structures: int = Field(..., gt=0)`), default values, and documentation. This Pydantic model *is* the specification for the application's configuration.
2.  **Define Internal Data Contracts:** Next, we define the models for data that flows internally, like `DFTResult`. This model guarantees that any component producing the result of a DFT calculation will always provide it in a consistent, predictable format.
3.  **Write Tests Against Schemas:** With the schemas defined, we can write unit tests for them. We test that valid YAML files are correctly parsed into `FullConfig` objects and that invalid files raise `ValidationError`. This TDD approach validates our "public API" before the application logic even exists.
4.  **Implement Logic:** Finally, we write the implementation code. Each function or class method is written with a clear understanding of the Pydantic models it will receive as input and the models it must produce as output. This completely eliminates a huge class of common bugs in scientific scripting, such as those arising from misspelled dictionary keys, incorrect data types, or forgetting to handle `None` values. The schemas serve as a powerful form of static analysis and runtime validation, making the entire application more robust and easier to debug and maintain. This disciplined, data-centric approach is a cornerstone of the MLIP-AutoPipe architecture.

## 5. Implementation Plan

The project will be developed over two distinct, sequential cycles. This iterative approach allows us to build and validate a stable core architecture before adding layers of scientific complexity, reducing risk and ensuring a solid foundation.

**Cycle 1: Core Framework and Basic Pipeline (Min 500 words)**
The user story for this cycle is: *"As a developer, I want to establish a runnable CLI and core architectural skeleton so that I have a stable, testable foundation for adding complex scientific features in the future."* The primary goal is not scientific accuracy but architectural soundness. We aim to create an end-to-end pipeline that executes a simplified workflow, proving that all the major components are correctly wired together. This cycle is about building the chassis of the car, ensuring the steering wheel connects to the wheels, before we drop in the high-performance engine.

The implementation will begin with setting up the project structure and dependencies in `pyproject.toml`, including `typer`, `pydantic`, `pyyaml`, and `ase`. The next step is the schema-first design: implementing all the Pydantic models in `config.py` and `common/pydantic_models.py`. This provides the data contracts for the rest of the implementation. Then, development moves to the `core/` components. The `AseDBWrapper` will be implemented first, with methods to create a new database and write `ase.Atoms` objects along with their associated data. This component is critical and can be tested in isolation. Next, we will implement a basic `AlloyGenerator`. This generator will be simple, creating a bulk alloy structure and randomly assigning atomic species according to the user's desired composition. It will, however, include the mandatory physical validation checks, establishing this pattern early.

The most critical piece of this cycle is the `WorkflowOrchestrator`. Its `run_workflow` method will be implemented to execute a simplified sequence: Generation -> Labeling -> Storage. The "Exploration" and "Sampling" stages will be intentionally skipped in this cycle's logic. To make the pipeline runnable, the `LabelingEngine` will be a simple mock. It will implement the correct interface (accepting an `ase.Atoms` object and returning a `DFTResult` object), but the returned object will contain dummy data, such as `energy=0.0`. This mock is crucial as it allows us to test the entire data flow of the orchestrator without introducing the complexity of managing external DFT codes. Finally, the `cli.py` module will be created. It will use `typer` to define the main `run` command, which takes a configuration file path, uses Pydantic to load and validate it, instantiates the `WorkflowOrchestrator`, and triggers the workflow. The key deliverable and definition of success for this cycle is the successful execution of the main integration test, which will run the CLI and verify the creation and content of the final database file.

**Cycle 2: Advanced Exploration and Full Feature Set (Min 500 words)**
The user story for this cycle is: *"As a materials scientist, I want to use the tool to run a full, scientifically meaningful data generation workflow, including realistic MD simulations and intelligent sampling, so that I can create a high-quality dataset for training my MLIP."* This cycle builds the "engine" and connects it to the chassis from Cycle 1. The focus shifts from architectural setup to the implementation of the complex scientific algorithms that provide the tool's core value.

The first major task is to replace the mock `LabelingEngine` with a real implementation. This `DFTLabelingEngine` will be a significant piece of engineering. It will need to manage external processes using `subprocess.run`, including creating temporary working directories for each calculation, generating correctly formatted input files for a specific DFT code (like Quantum Espresso), executing the code, and then robustly parsing the text-based output files to extract the required energy, forces, and stress data. This component must have extensive error handling to deal with DFT calculations that fail to converge. Next, the `MDMCExplorer` will be implemented. This involves leveraging ASE's MD libraries (like `Langevin` dynamics) and attaching the MLIP model (specified in the configuration) as the calculator that provides energies and forces. The hybrid MC logic will be added as a custom hook within the MD timestepping loop, attempting atom swaps at regular intervals.

Simultaneously, the `sampling` module will be built. This will include a simple `RandomSampler` and the more complex `FPSSampler`. The implementation of the `FPSSampler` will require integrating the `dscribe` library to compute SOAP descriptors for each structure in the MD trajectories. The `WorkflowOrchestrator` will then be upgraded to include the Exploration and Sampling stages in its sequence, calling these new components and passing the data between them correctly. Finally, the Web UI will be developed. The choice of Streamlit is strategic; it allows for the rapid development of a functional and interactive UI that is sufficient for the project's goals without the overhead of a full-stack JavaScript framework. The UI will essentially be a graphical front-end for generating the `FullConfig` Pydantic object, which it will then pass to the same `WorkflowOrchestrator` used by the CLI, ensuring consistent behavior across both interfaces. The successful completion of this cycle will result in a feature-complete application that fulfills all the initial project requirements.

## 6. Test Strategy

A rigorous, multi-layered testing strategy is essential for ensuring the correctness and robustness of the MLIP-AutoPipe application. The strategy is divided by cycle, mirroring the implementation plan, and combines comprehensive unit tests for component-level validation with integration tests for verifying the end-to-end workflow.

**Cycle 1 Test Strategy (Min 500 words)**
The testing strategy for Cycle 1 is focused on validating the core architecture, configuration handling, and the basic data pipeline. The goal is to build confidence in the foundational skeleton of the application before adding scientific complexity.

*   **Unit Tests:** We will use `pytest` for all testing. Each core component will have a dedicated test file in `tests/unit/`.
    *   **Configuration (`test_config.py`):** This is one of the most critical test suites. We will test the `FullConfig` Pydantic model exhaustively. Using `pyyaml`, we will load various YAML strings. We will have multiple tests for invalid configurations, asserting that a `pydantic.ValidationError` is raised in specific, predictable ways. For example, one test will check for missing required fields (e.g., omitting the `system` block entirely). Another will test for incorrect data types (e.g., `num_structures: "ten"`). A third will test for constraint violations (e.g., `num_structures: 0`, which violates the `gt=0` constraint). These tests ensure that our application is robust to user error and will fail fast with clear messages.
    *   **Generators (`test_generators.py`):** The `AlloyGenerator` will be tested for correctness. To ensure reproducibility in a test that involves randomness, we will set a fixed random seed (`numpy.random.seed`). We will then instantiate the generator with a specific configuration (e.g., a 50/50 Si-Ge alloy in an 8-atom cell). The test will assert that the output is a list of the correct length, that each `ase.Atoms` object in the list contains exactly 4 Si and 4 Ge atoms, and that the `overlap_check` has been successful (i.e., no two atoms are unphysically close).
    *   **Database Wrapper (`test_ase_db_wrapper.py`):** This test will use `pytest`'s `tmp_path` fixture to create a temporary database file, ensuring that tests do not leave artifacts on the file system. The test will instantiate the `AseDBWrapper` with this temporary path. It will then create a sample `ase.Atoms` object and a dummy `DFTResult` object and call the `write_atoms` method. The core of the test is the verification step: it will use `ase.db.connect` to read the data back from the temporary database and assert that the written data (e.g., energy, number of atoms, chemical symbols) exactly matches the data it intended to write. This validates the entire database interaction logic.

*   **Integration Tests (`test_cli_integration.py`):** While unit tests verify components in isolation, the integration test ensures they are all correctly wired together. We will use `typer.testing.CliRunner` to invoke our CLI application programmatically. The test function will first create a complete, valid `config.yaml` file in a temporary directory. It will then call `runner.invoke()`, passing the path to this config file. The test will perform a series of crucial assertions: 1) `result.exit_code == 0`, proving the application ran and exited successfully. 2) `result.exception is None`, confirming no unhandled exceptions occurred. 3) The specified output database file exists in the temporary directory. 4) The number of rows in the created database exactly matches the `num_structures` requested in the configuration file. This single end-to-end test provides extremely high confidence in the overall architecture.

**Cycle 2 Test Strategy (Min 500 words)**
The focus of Cycle 2 testing shifts to the correctness of the newly implemented scientific algorithms and their integration into the existing workflow.

*   **Unit Tests:**
    *   **Labeling Engine (`test_labeling.py`):** This component's interaction with the outside world (the DFT code) makes mocking essential. We will use `unittest.mock.patch` to completely mock the `subprocess.run` function. The mock will be configured to act as a stand-in for the real DFT code. We will create a library of realistic-looking but fake DFT output strings, some representing successful calculations and others representing common failure modes (e.g., convergence failure). Our tests will configure the mock to return these strings as `stdout`. We will then call our `LabelingEngine`'s parsing logic and assert that it correctly extracts the energy and forces from the "successful" string and that it raises a custom `DFTRuntimeError` when presented with the "failed" string. This allows us to test our parsing and error-handling logic exhaustively without any dependency on an actual DFT installation.
    *   **Samplers (`test_sampling.py`):** Testing the `FPSSampler` requires a carefully constructed test case. We will create a synthetic trajectory of `ase.Atoms` objects. For example, the first 10 frames will be nearly identical (high structural similarity), while the next 5 frames will be deliberately made very different from the first 10 and from each other. We will then configure the `FPSSampler` to select 5 structures from this trajectory. A correct implementation of FPS *must* select the 5 distinct structures. The test will assert that the indices of the selected structures match the known distinct ones. This provides a clear, unambiguous validation of the core sampling algorithm.
    *   **Exploration Engine (`test_exploration.py`):** Unit testing the full MD engine is difficult, but we can test its key decision-making logic. The most important is the `auto_ensemble` feature. We will write a test that creates two `ase.Atoms` objects: one with `pbc=True` in all three dimensions (a bulk system) and one with `pbc=[True, True, False]` (a surface slab). We will pass each of these to the function responsible for choosing the ensemble and assert that it returns 'npt' for the bulk object and 'nvt' for the slab object.

*   **Integration Tests (`test_full_pipeline_integration.py`):** A new end-to-end integration test will be created to validate the full, complex pipeline. To ensure the test can run quickly and reliably in a CI environment, we will make several compromises: we will use a very small atomic system (e.g., 4 atoms), a very short MD run (e.g., 20 steps), and we will continue to mock the `LabelingEngine` to avoid external dependencies. The test will configure the workflow to use the `hybrid_md_mc` explorer and the `fps` sampler. It will invoke the CLI and assert that the entire process runs to completion without errors. The key verification will be on the final database: the test will assert that the number of rows in the database equals the number of samples requested by FPS (not the number of MD steps), confirming that the new Exploration and Sampling stages were correctly integrated and executed by the `WorkflowOrchestrator`.

*   **Manual Testing (Web UI):** The Web UI will be tested manually. The plan will list a series of steps for a human tester to follow, such as launching the app, using the UI widgets to define a configuration, starting a run, and verifying that the output database is created and has the correct content, matching the results of an equivalent CLI run.
