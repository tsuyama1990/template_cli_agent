# SYSTEM ARCHITECTURE: MLIP-AutoPipe

## 1. Summary

MLIP-AutoPipe is a highly automated and robust framework designed to generate high-quality, physically valid, and diverse datasets for training modern Machine Learning Interatomic Potentials (MLIPs), such as MACE and SevenNet. The core philosophy of the project is to "remove the human expert from the loop" by creating a system that intelligently explores the thermodynamic phase space of a given material system to produce training data. The development of accurate MLIPs is a critical enabler for large-scale, high-fidelity materials simulations, but the quality of these models is entirely dependent on the quality of the training data they are fed. Traditionally, generating this data is a significant bottleneck. It often involves computationally expensive *ab initio* calculations (like Density Functional Theory) on a manually selected set of atomic configurations. This process is not only slow and costly but also highly reliant on the intuition and experience of a domain expert to choose structures that adequately cover the potential energy surface. The risk is that a biased or incomplete dataset will lead to an MLIP that is unreliable when used for simulations, especially for predicting phenomena like phase transitions or defect dynamics. MLIP-AutoPipe directly addresses this challenge by replacing the manual, ad-hoc approach with a systematic, reproducible, and efficient automated pipeline.

The system is engineered to handle a wide variety of physical systems, acknowledging the diverse needs of the materials science community. Its capabilities will extend to multi-component alloys, ionic crystals (e.g., oxides and salts), covalent materials, complex interfaces between different materials (heterostructures), and surface adsorption phenomena. This versatility is achieved through a modular, factory-based design that allows different physics-based generators to be used interchangeably. The primary output of the pipeline is not just a collection of raw data files, but a fully curated and structured database of atomic configurations. Each structure is stored with rich metadata, including its calculated energy and forces, the simulation parameters that produced it, and its lineage, making the dataset ready for immediate use in MLIP training workflows and for subsequent analysis.

The architecture is conceived as a robust, sequential pipeline comprising four distinct stages: Generation, Exploration, Sampling, and Storage. The **Generation** stage is the entry point, responsible for creating a set of initial "seed" structures based on high-level user inputs. A critical feature of this stage is the immediate application of physics-based validation checks, such as ensuring that no two atoms are unrealistically close. This prevents wasted computational effort on physically impossible starting points. The **Exploration** stage is the computational core of the pipeline. It takes the seed structures and evolves them using Molecular Dynamics (MD) and hybrid Monte Carlo (MC) simulations. This process is analogous to gently heating and shaking the material to see what configurations it can adopt, allowing the system to discover a vast range of atomic structures, including the high-energy and transition-state configurations that are often the most difficult, and therefore most important, for an MLIP to learn.

Following exploration, the **Sampling** stage intelligently filters the enormous number of structures generated. Since adjacent steps in a simulation are highly correlated, simply using all of them is inefficient. This stage employs advanced algorithms like Farthest Point Sampling (FPS) to select a subset of structures that are maximally diverse, ensuring the final dataset is information-rich and compact. Finally, the **Storage** stage is responsible for persisting the curated dataset. It takes the sampled structures, computes any final properties, and saves them into a structured, queryable ASE (Atomic Simulation Environment) database, which is built on SQLite for portability and ease of use. A key, overarching design principle is robustness and fault tolerance. The system is built for long-running, potentially failure-prone simulations. Features like atomic checkpointing after each stage, graceful handling of simulation crashes, and sophisticated safety mechanisms (e.g., preventing "Coulomb explosions" in ionic systems) are woven into the fabric of the architecture. The entire system is highly configurable via the Hydra framework, providing a clear, powerful, and flexible interface for users. While the primary interface is a command-line tool designed for batch processing and integration into automated high-throughput workflows, a secondary Web UI will also be developed for interactive use, making the power of the tool accessible to a broader audience.

## 2. System Design Objectives

The design of MLIP-AutoPipe is guided by a set of clear and ambitious objectives aimed at creating a powerful, flexible, and truly automated tool for materials scientists and MLIP developers. These objectives are categorised into primary goals that define the system's purpose, and technical goals that define its engineering quality.

**Primary Objectives:**
- **Full Automation:** The foremost objective is to completely automate the end-to-end process of generating MLIP training data. The system should require only high-level inputs from the user, such as the chemical formula and target material class. It must then autonomously handle the complex, multi-step workflow of generating initial structures, running extensive simulations, selecting informative samples, and storing the results without needing manual intervention. This includes automating key decisions, such as choosing the appropriate simulation ensemble (NVT vs. NPT) based on the system's geometry, which is a prime example of removing the need for expert-level user input. The ultimate vision is a "fire-and-forget" tool that can be integrated into larger, high-throughput materials discovery platforms.
- **Data Quality and Physical Validity:** The integrity of the output dataset is paramount. Every structure generated by the pipeline must be physically realistic and relevant. The system will enforce a series of rigorous physical constraints at every stage. In the generation phase, this means checking for steric clashes (overlapping atoms). In the exploration phase, it involves using mixed potentials (e.g., MLIP + ZBL) to correctly model high-energy repulsive interactions and prevent atoms from fusing. For ionic systems, charge neutrality must be enforced. The goal is to produce a dataset that accurately represents the true potential energy surface of the material, which is the bedrock upon which a reliable MLIP is built.
- **Maximisation of Structural Diversity:** A successful MLIP must be generalisable, meaning it can accurately predict energies and forces for configurations it has never seen during training. This requires the training data to be exceptionally diverse, covering a wide range of local atomic environments, coordination numbers, temperatures, and pressures. MLIP-AutoPipe is explicitly designed to maximize this diversity. The hybrid MD/MC exploration engine is the primary tool for this, as it can overcome energetic barriers that would trap a pure MD simulation. This is complemented by the Farthest Point Sampling (FPS) algorithm, which ensures the final dataset is an efficient and unbiased representation of the entire explored configuration space, prioritising rare and unique structures over redundant, low-energy ones.
- **Robustness and Reliability:** The exploration phase involves orchestrating what could be thousands of independent, long-running simulations. In such a scenario, failures are not an exception; they are an expectation. The system must therefore be exceptionally robust. This is achieved through multiple layers of defence. Firstly, checkpointing between each major stage allows the pipeline to be resumed from the point of failure, saving hours or days of computation. Secondly, individual simulation crashes are handled gracefully; the system will log the error, save the failing structure for debugging, and continue processing the remaining simulations. Thirdly, preventative measures, like the charge-swap safety check for ionic systems, are built-in to avoid common failure modes like Coulomb explosions.

**Technical Objectives:**
- **Modularity and Extensibility:** The architecture is fundamentally modular. The pipeline stages are decoupled, and the components within each stage (generators, explorers, samplers) are designed as interchangeable "plugins". This is enforced through a strict separation of interfaces (defined as Abstract Base Classes) from their concrete implementations. This design makes the system highly extensible. Adding support for a new material class or a new exploration algorithm will involve writing a new, self-contained service class and updating a factory configuration, without requiring any changes to the core pipeline orchestrator.
- **Performance and Scalability:** The framework must be computationally efficient to be practical. It is designed for parallel execution from the ground up, using Python's `ProcessPoolExecutor` to distribute simulations across all available CPU cores. For GPU-accelerated MLIPs, it will manage resources carefully. A key performance optimisation is the "late binding" of the MLIP calculator. Large ML models are expensive to serialise and pass between processes. By instantiating the calculator only within the worker process where it is needed, we bypass this significant overhead, leading to faster startup times and lower memory usage.
- **Usability and Configuration:** A powerful tool is only useful if its capabilities are accessible. MLIP-AutoPipe will cater to two distinct user profiles. For power users and automated workflows, a comprehensive Command-Line Interface (CLI) will provide full control over the pipeline. System configuration is managed by the Hydra framework, which provides a clean, composable, and overridable YAML-based format, with all parameters validated by Pydantic for correctness. For users who prefer a more interactive approach, a secondary Web UI will provide a guided experience for configuring and launching runs, lowering the barrier to entry.

**Success Criteria:**
The success of the project will be measured against these objectives. We will benchmark the system by generating a dataset for a well-known material (e.g., Silicon) and training a MACE model with it; the model's accuracy must be comparable to one trained on an established benchmark dataset. The system's versatility will be proven by successfully executing end-to-end runs for at least three distinct material classes (alloy, ionic, surface adsorption) using only configuration changes. Its performance and stability will be demonstrated by completing runs for large systems (>200 atoms) without crashing. Finally, the modularity will be validated by a practical test: implementing a new, simple `CovalentGenerator` (e.g., for graphene) should be achievable in a short timeframe and without modifying the core pipeline code.

## 3. System Architecture

The MLIP-AutoPipe framework is architected as a modular, four-stage pipeline orchestrated by a central `PipelineRunner`. This design promotes a clear and sequential flow of data, with strong separation of concerns between the stages. To ensure robustness and fault tolerance, the outputs from each stage are persisted to the file system and a central database, creating atomic checkpoints. This allows the pipeline to be stopped and resumed without loss of work, and it decouples the stages, meaning a later stage can be re-run with different parameters without having to repeat the earlier, computationally expensive stages.

```mermaid
graph TD
    A[Start] --> B{1. Generation};
    B --> C{2. Exploration};
    C --> D{3. Sampling};
    D --> E{4. Storage};
    E --> F[End];

    subgraph "User Input"
        G[Hydra Config YAML]
    end

    subgraph "Pipeline Stages"
        B; C; D; E;
    end

    subgraph "Data Persistence Layer"
        H[(ASE SQLite Database)];
        I[/file/system/xyz_files];
    end

    G -- Defines workflow --> B;
    B -- Seed Structures (.xyz) --> I;
    B -- Records generation event --> H;
    I -- Feeds initial structures --> C;
    C -- Trajectory data (.xyz) --> I;
    I -- Provides trajectories for sampling --> D;
    D -- Curated Structures (.xyz) --> I;
    I -- Provides final structures --> E;
    E -- Stores final structures & metadata --> H;
```

**Component Breakdown:**

1.  **Configuration (Hydra & Pydantic):** The entire workflow is controlled by a set of YAML configuration files. Hydra is used to compose these files and allow for easy overriding of parameters from the command line. This is a critical feature for a scientific tool, as it allows users to run systematic experiments (e.g., scanning a range of temperatures) without changing the configuration files. We leverage Pydantic to define a strict schema for these configurations. When Hydra loads the YAML files, the data is immediately parsed into Pydantic models, which provides a powerful validation layer. This catches user errors (typos, incorrect data types, values out of range) at the very beginning of the process, preventing a long simulation from failing hours later due to a simple mistake.

2.  **`PipelineOrchestrator`:** This is the central controller or the "brain" of the application. It is responsible for reading the validated configuration object, instantiating the necessary service components for each stage using factories, and executing the four stages in the correct sequence. It manages the overall state of the workflow. For example, before running the Generation stage, it checks if the `initial_structures.xyz` file already exists. If it does, the orchestrator skips that stage and proceeds directly to Exploration, making the pipeline idempotent and resumable. It acts as the glue that connects the stages, passing data (primarily file paths) from one to the next.

3.  **Stage 1: Generation:** This stage is responsible for creating the initial set of atomic structures. A `GeneratorFactory` inspects the configuration and instantiates the appropriate generator class (e.g., `AlloyGenerator`, `IonicGenerator`), which must conform to the `IGenerator` interface. The selected generator then creates a list of ASE `Atoms` objects. These are immediately passed to a validation routine that enforces fundamental physical constraints, such as checking for atoms that are too close (a common issue with random structure generation). Only the validated, physically plausible structures are saved to disk and logged in the database.

4.  **Stage 2: Exploration:** This is the most computationally intensive stage. The `ExplorerEngine` reads the seed structures and distributes them among a pool of worker processes for parallel execution. Each worker process is responsible for running a simulation (e.g., MD or hybrid MD/MC) on a single structure. A critical design choice here is the "late binding" of the MLIP calculator. Instead of creating the large, complex calculator object in the main process and trying to serialize it to the workers (which is inefficient and often fails for PyTorch models), the worker process itself is responsible for instantiating the calculator. This minimizes inter-process communication overhead and improves scalability. The engine also contains sophisticated logic for automatically selecting the correct thermodynamic ensemble and for mixing potentials to ensure simulation stability. Trajectories are streamed to disk progressively to prevent data loss.

5.  **Stage 3: Sampling:** The exploration stage produces a massive amount of highly correlated data. The `Sampler` component's role is to process these raw trajectories and distill them into a smaller, more informative, and less redundant dataset. The `SamplerFactory` will instantiate the chosen sampling strategy (e.g., `RandomSampler` or `FarthestPointSampler`). The sampler reads the trajectory files, computes descriptors if necessary, and applies its algorithm to select the final set of structures, which are then written to a new set of curated `.xyz` files.

6.  **Stage 4: Storage:** The final stage takes the curated structures and archives them permanently in the central ASE database. The `Storage` service iterates through the sampled structures, performs any final calculations (e.g., ensuring forces are present), and writes each one as a new row in the database. Crucially, it stores rich metadata alongside each structure, such as its energy, forces, stresses, and key parameters from its simulation history (e.g., the temperature at which it was generated). This turns the output from a simple collection of coordinates into a queryable, analysis-ready scientific database.

**Data Flow:**
The primary data entities are ASE `Atoms` objects. The communication and data handoff between the major pipeline stages are intentionally done via the file system (`.xyz` files). An `Atoms` object is generated, saved to a file, and then the path to that file is passed to the next stage. This file-centric approach, combined with the database for logging state, makes the system robust. It ensures the stages are loosely coupled and that the state of the workflow is always recoverable from the persisted files, even after a system crash.

## 4. Design Architecture

The software design of MLIP-AutoPipe is founded on established principles of modern software engineering to ensure it is maintainable, testable, and extensible. The architecture emphasizes a clear separation of concerns, enforced through a layered structure and the use of dependency inversion.

**File Structure (High-Level):**
The project's directory structure is a physical manifestation of its logical architecture. Each directory has a clearly defined purpose, which helps developers navigate the codebase and understand the role of each component.
```
src/mlip_autopipec/
├── cli/                      # Command-Line Interface Layer
│   └── main.py
├── core/                     # Orchestration and Core Logic Layer
│   ├── factories.py
│   ├── interfaces.py
│   └── pipeline_orchestrator.py
├── domain/                   # Data Structures and Schemas Layer
│   ├── configuration.py
│   └── data_models.py
├── services/                 # Business Logic and Implementation Layer
│   ├── generation/
│   ├── exploration/
│   ├── sampling/
│   └── storage/
├── utils/                    # Shared, Stateless Utilities
│   └── physics.py
└── gui/                      # Presentation Layer (Web UI)
    └── main_gui.py
```

**Key Design Patterns and Principles:**

*   **Dependency Inversion & Interfaces:** This is the most critical principle for achieving a modular and extensible system. The high-level orchestration logic in the `core` directory does not depend on the low-level implementation details in the `services` directory. Instead, both depend on abstractions defined in `core/interfaces.py`. For example, the `PipelineOrchestrator` doesn't know how to create an alloy structure; it only knows how to call the `generate()` method on an object that implements the `IGenerator` interface. This is dependency inversion. It decouples the "what" from the "how". This means we can add a completely new `IonicGenerator` in Cycle 2, and the `PipelineOrchestrator` will work with it seamlessly without a single line of its code needing to change. This makes the system open for extension but closed for modification.

*   **Factory Pattern:** The Factory pattern is used to bridge the gap between the abstract interfaces and their concrete implementations. The `factories.py` module contains classes (e.g., `GeneratorFactory`, `SamplerFactory`) whose sole responsibility is to instantiate the correct service class based on the user's configuration. The factory takes the Pydantic configuration object, inspects a `name` field (e.g., `name: "alloy"`), and returns an instance of the corresponding class (`AlloyGenerator`). This pattern centralises the object creation logic, preventing it from being scattered throughout the application and making it easy to register new implementations.

*   **Pydantic for Schemas and Validation:** Pydantic is used as a foundational technology for defining all data schemas, acting as a strong "type and validation layer" at the boundaries of the system. Its primary use is in `domain/configuration.py` to model the Hydra configuration, providing immediate and descriptive feedback to the user about any errors. This prevents invalid configuration data from ever entering the application's core logic. Furthermore, Pydantic will be used in `domain/data_models.py` to define internal data structures, such as the results packet from a simulation run. This ensures that data passed between different parts of the application is always well-formed and type-safe, which drastically reduces the potential for runtime bugs. By using `extra="forbid"`, we also protect against misspelled keys, a common source of error in YAML files.

*   **Layered Architecture / Separation of Concerns:** The file structure clearly defines a layered architecture where each layer has a distinct responsibility.
    *   **`domain/` (Data Layer):** This layer knows nothing about business logic. It simply defines the shape and constraints of the data used in the application.
    *   **`services/` (Business Logic Layer):** This layer contains the concrete implementations of all the core tasks (the "how"). The modules in this layer are self-contained and focus on a single job, such as generating alloys or running MD. They should not contain orchestration logic.
    *   **`core/` (Orchestration Layer):** This layer is responsible for the high-level workflow (the "when" and "in what order"). It orchestrates the services but does not contain any specific business logic itself. It operates solely on the abstract interfaces.
    *   **`cli/` & `gui/` (Presentation Layer):** These layers are the entry points for the user. Their job is to handle user input and display output. They delegate all the real work to the orchestration layer and should contain minimal logic.

This strict separation ensures that the code is easy to reason about, maintain, and test. A change in the business logic for sampling shouldn't require a change in the CLI, and a change in the CLI shouldn't affect the core orchestration.

## 5. Implementation Plan

The project will be developed over two distinct cycles, each building upon the last. This iterative approach allows us to deliver a functional core product quickly and then progressively enhance it with more advanced and complex features.

**CYCLE01: Core Pipeline and CLI Foundation**

This foundational cycle is focused on building a robust, end-to-end command-line tool that is fully functional for a single, important class of materials: alloys. The objective is not to deliver all the advanced features, but to establish a solid architectural backbone that can support future development. This includes creating the core orchestration logic, the data persistence layer, the configuration system, and all the necessary interfaces. The user-facing deliverable will be a working CLI that can take a configuration file for an alloy and produce a valid training dataset in an ASE database.

The implementation will begin by defining the project's dependencies and structure in `pyproject.toml`. The next critical step is to define the Pydantic schemas in `domain/configuration.py`, as these models serve as the data contract for the entire application. With the contracts in place, development will proceed on the isolated service components. The `AseDBWrapper` is a good starting point, as it has no other internal dependencies. Next, the `AlloyGenerator` will be implemented, leveraging the validation utilities from `utils/physics.py`. The `RandomSampler`, being simple, will follow. The most significant component of this cycle is the `MDEngine`. Its implementation will focus on correctly setting up the ASE dynamics objects and, most importantly, on robustly managing the parallel execution via `ProcessPoolExecutor`, including the "late-binding" of the MLIP calculator to ensure performance and avoid serialisation errors. With the services implemented, the factories will be built in `core/factories.py` to connect the configuration to the implementations. Finally, the `PipelineOrchestrator` will be assembled to control the workflow, and the `cli/main.py` file will be written to provide the user entry point, tying everything together. This systematic, bottom-up approach ensures that each component is built on a solid and tested foundation.

**Key Features for CYCLE01:**
1.  **CLI Development:** Implement the main entry point using `click`, providing a command to run the full pipeline from a Hydra configuration directory.
2.  **Configuration System:** Set up the complete Hydra configuration structure, backed by the Pydantic models for rigorous, type-safe validation of all user settings.
3.  **Pipeline Orchestrator:** Build the central `PipelineOrchestrator` class that manages the sequential execution of the four stages and handles the checkpointing/resumption logic.
4.  **Database Wrapper:** Create a robust `AseDBWrapper` service that encapsulates all interactions with the ASE database, providing clean methods for creating the database and storing atomic structures and metadata.
5.  **Alloy Generator:** Implement the first structure generator, `AlloyGenerator`, for creating multi-component random alloy structures with specified crystal lattices and compositions.
6.  **Basic MD Explorer:** Implement a basic `MDEngine` that uses an MLIP calculator to run NVT Molecular Dynamics simulations at a fixed temperature, parallelised across multiple initial structures.
7.  **Random Sampler:** Implement a simple `RandomSampler` that randomly selects a specified number of frames from the generated MD trajectories.
8.  **Comprehensive Test Suite:** Develop a full suite of unit and integration tests covering all the core components to ensure the reliability and correctness of the foundational pipeline.

**CYCLE02: Advanced Exploration and User Interface**

Building on the stable foundation of Cycle 1, this cycle introduces a suite of advanced features that dramatically increase the power, versatility, and accessibility of the tool. The focus shifts from core functionality to sophisticated physical modeling and user experience. The central technical achievement will be the upgrade of the `MDEngine` to a full `HybridMDMC_Engine`. This is a complex but crucial feature that enables much more efficient exploration of the potential energy surface by allowing atoms to swap identities, a move that is essential for modeling disordered alloys and finding new low-energy configurations.

The development will also broaden the applicability of the tool. New generators, starting with an `IonicGenerator`, will be added to support different classes of materials. This will require careful implementation to handle constraints like charge neutrality. The exploration engine will be further enhanced with automatic ensemble switching, a feature that adds a layer of intelligence by allowing the system to use the correct simulation type for bulk vs. slab models. The data quality will be improved by implementing Farthest Point Sampling (FPS), a more intelligent method for selecting diverse structures, replacing the naive random sampling of the first cycle. A significant user-facing improvement will be the development of a web-based GUI. This will provide an interactive and visual way to configure and run the pipeline, making the tool accessible to users who are not comfortable with a command-line-driven workflow. This involves creating a separate web application (likely using Streamlit) that can generate the necessary Hydra configuration files and launch the backend pipeline process. The implementation of these features will follow the established pattern: define the Pydantic config, create the service, update the factory, and then write the tests. This modular approach allows for these complex features to be added incrementally without disrupting the existing, working system.

**Key Features for CYCLE02:**
1.  **Hybrid MD/MC Engine:** Upgrade the explorer to a hybrid engine that combines standard MD integration steps with Monte Carlo moves, specifically atom swaps, to efficiently explore the configuration space of alloys.
2.  **Advanced Generators:** Implement additional generators for other material classes, starting with an `IonicGenerator` for systems like oxides and salts, which will enforce charge neutrality constraints.
3.  **Automatic Ensemble Switching:** Implement logic within the explorer to automatically detect the presence of a vacuum layer in the simulation cell and dynamically switch between the NPT (for bulk systems) and NVT (for slabs/surfaces) ensembles.
4.  **Farthest Point Sampling (FPS):** Implement the FPS algorithm as a new, more intelligent sampling strategy to maximize the structural diversity of the final dataset, using SOAP descriptors to characterize atomic environments.
5.  **Web UI:** Develop a graphical user interface using a web framework like Streamlit that allows users to visually configure all pipeline parameters, launch a run, and see a summary of the results, including visualisations of the generated structures.
6.  **ZBL Potential Integration:** Enhance the explorer by adding the capability to mix the primary MLIP with a classical ZBL potential to correctly model the strong repulsive interactions at very short interatomic distances, preventing simulation failures at high temperatures.
7.  **Expanded Test Suite:** Augment the existing test suite with new unit and integration tests that specifically target and validate the correctness and effectiveness of all the new advanced features.

## 6. Test Strategy

The project will adhere to a rigorous, multi-layered testing strategy to ensure code quality, correctness, and robustness from the outset. We will use `pytest` as our testing framework and follow best practices, including continuous integration, code coverage monitoring, and static analysis.

**CYCLE01 Test Strategy:**
The testing focus in the first cycle is on building a solid and reliable foundation. The goal is to ensure that every component of the core pipeline is thoroughly validated, both in isolation and as part of the integrated whole.
*   **Unit Testing:** Each module will have a corresponding test file (e.g., `test_alloy.py` for `alloy.py`). We will use `pytest-mock` extensively to isolate components from their dependencies. For example, when testing the `PipelineOrchestrator`, we will mock the `IGenerator`, `IExplorer`, `ISampler`, and `IStorage` interfaces. This allows us to verify that the orchestrator calls the correct methods in the correct sequence with the correct arguments, without needing to run a real simulation or write to a real database. We can simulate various scenarios, such as a file already existing, to test the checkpointing logic. For the `AlloyGenerator`, we will test that it produces structures with the correct number of atoms and the specified composition. We can even mock the random number generator to make the output deterministic and thus perfectly testable. For the Pydantic configuration models, we will write tests that attempt to create instances with invalid data (e.g., negative temperature, composition not summing to 1.0) and assert that a `ValidationError` is raised, ensuring our validation rules are effective.
*   **Integration Testing:** While unit tests verify the components in isolation, integration tests ensure they work together correctly. The primary integration test for CYCLE01 will be an end-to-end run of the CLI on a small, well-defined alloy system. This test, orchestrated by a `pytest` function using `click.testing.CliRunner`, will execute the entire pipeline in a temporary directory. To make this test fast enough for a CI environment, we will configure it to use a computationally cheap, non-ML calculator, such as ASE's built-in EMT potential, and run a very short simulation (e.g., 20 steps). After the CLI command completes, the test will perform a comprehensive set of assertions. It will check that all intermediate files were created. It will then connect to the final output database and verify its contents: does it have the correct number of rows? Do the rows contain the expected data, such as energy and forces? This single test provides a high degree of confidence that the entire system, from argument parsing to data storage, is functioning as designed.

**CYCLE02 Test Strategy:**
The strategy for Cycle 2 is to expand the test suite to cover all the new, advanced functionalities, with a particular focus on verifying not just their correctness but also their *effectiveness*.
*   **Unit Testing:** New unit tests will be developed for each new feature. The logic for the `HybridMDMC_Engine` will be carefully tested. We can test the MC swap logic by creating a simple 2-atom system, running the swap function, and asserting that the atoms' chemical symbols have been exchanged. The automatic ensemble switching logic will be tested by passing it pre-constructed `Atoms` objects—one representing a bulk crystal and one with a large vacuum gap—and asserting that the function returns the correct ensemble type (`'npt'` or `'nvt'`). The `FarthestPointSampler` is a critical algorithm to test. We will create a deterministic unit test by feeding it a simple, known set of 2D points where the most diverse subset is obvious, and we will assert that the FPS implementation correctly selects that exact subset. For the `IonicGenerator`, tests will assert that every generated structure is perfectly charge-neutral.
*   **Integration Testing:** The integration test suite will be extended with new end-to-end scenarios. A key new test will be a *comparative* test for the FPS sampler. This test will run the pipeline up to the exploration stage to generate a set of trajectories. Then, it will run the sampling/storage stages twice: once with the `random` sampler and once with the `fps` sampler. The test will then load both resulting datasets and perform a quantitative analysis, for example, by calculating the standard deviation of the pairwise distances between the SOAP descriptors of all structures in each set. The core assertion will be that the diversity metric for the FPS-generated dataset is significantly higher than for the random one, providing quantitative proof of the feature's effectiveness. Another integration test will be designed for the hybrid engine, which will analyze the output trajectory to confirm that the elemental identities of atoms have indeed changed over time, proving the swap moves are working. The Web UI will be manually tested to ensure usability, and an automated test will be added to confirm that it can correctly generate a config file and launch the backend pipeline process successfully.

**Overall Test Infrastructure:**
*   **Continuous Integration (CI):** All tests (unit and integration) will be executed automatically on every push and pull request using GitHub Actions. This ensures that regressions are caught immediately.
*   **Code Coverage:** We will use `pytest-cov` to monitor the percentage of the codebase covered by tests. We will aim for a high coverage percentage and pay special attention to ensuring that all complex logic branches are tested.
*   **Static Analysis:** We will use `ruff` for automated linting and code formatting. This will enforce a consistent code style and catch a wide range of potential bugs and anti-patterns before the code is even run.
