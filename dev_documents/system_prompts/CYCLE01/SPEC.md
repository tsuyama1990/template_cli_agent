# Cycle 01 Specification: Core Engine & Workflow Foundation

## 1. Summary

Cycle 01 marks the foundational stage of the MLIP-AutoPipe project. The primary objective of this cycle is to construct the essential backbone of the automated pipeline. This involves creating a robust system that can take a user-provided set of atomic structures, automatically calculate their energies, forces, and stresses using a powerful first-principles engine, and then use this labelled data to train a Machine Learning Interatomic Potential (MLIP). This cycle deliberately scopes out the complexities of automatic structure generation and active learning to focus entirely on establishing a reliable and repeatable core workflow. The key deliverables are a functional `LabellingEngine` and `TrainingEngine`, orchestrated by a nascent `WorkflowOrchestrator`. This initial phase is arguably the most critical, as it validates the core assumptions of the project: that the chosen external software tools (Quantum Espresso for DFT, and the ACE framework for MLIPs) can be programmatically controlled in a robust and scalable manner. Success in this cycle means we have a solid, testable foundation upon which all future intelligence and automation can be built. It involves tackling the non-trivial challenges of interfacing with legacy scientific codes, parsing their complex text-based outputs, and managing a data-centric workflow where the integrity and provenance of every calculation are paramount.

The `LabellingEngine` will be built as a sophisticated wrapper around the Quantum Espresso (QE) software package. Its role is to abstract away the complexity of setting up and running DFT calculations. It will automatically generate the necessary QE input files from `ase.Atoms` objects, manage the execution of the `pw.x` binary, parse the output files to extract the required data, and handle common runtime errors. A significant part of this module's development will be dedicated to implementing robust error recovery mechanisms, ensuring that the pipeline can gracefully manage issues like SCF convergence failures without manual intervention. This includes not just detecting errors, but attempting automated fixes, such as adjusting mixer parameters or smearing values, which codifies the trial-and-error process an expert user would typically perform manually. The engine will be responsible for ensuring that every piece of data entering the training set is of high quality and correctly formatted.

The `TrainingEngine` will be responsible for the MLIP creation process itself. For this cycle, we will use the Atomic Cluster Expansion (ACE) model as the learning framework. The engine will take the labelled dataset produced by the `LabellingEngine` and prepare it for training. A key feature to be implemented from the outset is "delta learning." Instead of training the model on the raw DFT values, it will learn the difference (delta) between the DFT calculations and a simpler, physics-based baseline potential (like Lennard-Jones). This technique is known to improve model accuracy and data efficiency by allowing the flexible machine learning model to focus its capacity on capturing the complex, quantum-mechanical interactions that the simple baseline potential misses, rather than wasting capacity learning basic physics like electrostatic repulsion.

Finally, the `WorkflowOrchestrator` will be introduced to manage the flow of data between these two core engines. It will read a configuration file, identify the structures that require labelling from a central database, pass them to the `LabellingEngine`, and then channel the resulting labelled data to the `TrainingEngine`. The project's structure, dependency management using `uv` and `pyproject.toml`, and the fundamental data storage mechanism via an ASE database will all be established during this cycle. This provides a solid, modern software engineering foundation for the scientific workflow, ensuring reproducibility, maintainability, and scalability from day one. The successful completion of this cycle will result in a functional, albeit not yet fully autonomous, pipeline capable of producing a bespoke MLIP from a user-curated set of atomic configurations.

## 2. System Architecture

In Cycle 01, the system architecture is a simplified, linear version of the final design, focusing exclusively on the core data processing pathway. It comprises three main software components: the `WorkflowOrchestrator`, the `LabellingEngine` (Module C), and the `TrainingEngine` (Module D), with the ASE Database serving as the central, persistent data repository. The user's role in this cycle is to initiate the process by providing two key inputs: a detailed configuration file specifying all calculation and training parameters, and an ASE database already populated with the atomic structures that need to be calculated. This deliberate simplification allows us to focus on the robustness of the data flow and the integration with external tools without the added complexity of automated data generation.

The process flow is designed to be sequential, transactional, and restartable:
1.  **Initiation:** The `WorkflowOrchestrator` is invoked via the command-line interface. Its first action is to load and validate the user-provided configuration file. This ensures that all required parameters are present and correctly formatted before any computation begins.
2.  **Data Retrieval:** The orchestrator connects to the ASE Database specified in the configuration. It then executes a query to identify all atomic structures that are marked with a 'needs_labelling' status. This status-based queuing system is fundamental to the architecture, as it allows the pipeline to be stopped and restarted without losing track of which tasks have been completed.
3.  **Delegation to Labelling Engine:** The orchestrator iterates through the list of retrieved structures. For each structure, it delegates the computationally intensive task of labelling to the `LabellingEngine`. This is a one-to-one mapping: one structure is passed to the engine, and the orchestrator waits for the result before proceeding to the next.
4.  **DFT Calculation (Labelling Engine):** The `LabellingEngine` is where the interaction with the external Quantum Espresso software occurs. Upon receiving an `ase.Atoms` object, it performs a series of internal steps:
    a. It generates a syntactically correct Quantum Espresso input file based on the `Atoms` object's properties (atomic positions, species, cell dimensions) and the DFT parameters from the configuration (cutoffs, k-points, etc.).
    b. It executes the `pw.x` binary as a separate process using Python's `subprocess` module. This is a critical integration point, and the engine is responsible for managing the execution environment, capturing standard output and error streams, and monitoring the process for completion or failure.
    c. Upon completion, the engine meticulously parses the voluminous QE output file to locate and extract the key scalar and vector quantities: the total energy, the forces on each atom, and the global stress tensor.
    d. It then attaches this extracted data back to the original `ase.Atoms` object by creating and assigning an ASE `SinglePointCalculator`. This is a crucial step for data encapsulation, as it keeps the structure and its calculated properties together.
5.  **Database Update:** The `LabellingEngine` returns the updated `Atoms` object to the orchestrator. The orchestrator then updates the structure's corresponding record in the ASE Database, storing the newly calculated results and, critically, changing its status from 'needs_labelling' to 'labelled'. This transactional update marks the successful completion of the labelling task for that structure.
6.  **Delegation to Training Engine:** Once the loop over all structures is complete, the `WorkflowOrchestrator` proceeds to the training stage. It performs a new query on the database, this time retrieving all structures marked as 'labelled'. This complete, high-fidelity dataset is then passed in its entirety to the `TrainingEngine`.
7.  **MLIP Training (Training Engine):** The `TrainingEngine` performs the machine learning task:
    a. It first preprocesses the data. This involves calculating the "delta" for each structureâ€”the difference between the DFT-calculated energy/forces and the energy/forces predicted by a simpler baseline potential.
    b. It then formats this delta dataset into the specific data structures required by the underlying ACE model training library.
    c. It calls the library's fitting function to train the potential.
8.  **Model Persistence:** Once the training is complete, the `TrainingEngine` saves the resulting potential as a file artifact (e.g., `model.ace`). The orchestrator records the path to this file, marking the successful completion of the entire workflow.

This architecture ensures a clear separation of concerns. The orchestrator handles the high-level workflow logic ("what to do"), while the engines encapsulate the specific technical details of how to perform each task ("how to do it"). The database acts as a durable, transactional intermediary, making the entire pipeline resilient to failures and interruptions. The use of a modern project setup with `pyproject.toml` and `uv` ensures that the development environment is reproducible and that dependencies are managed cleanly from the very beginning.

## 3. Design Architecture

The design for Cycle 01 focuses on creating a set of well-defined classes and data models to implement the core workflow. This object-oriented approach ensures encapsulation, testability, and maintainability. The implementation will be housed within the `src/mlip_autopipec` package, establishing a clear namespace for the project from the outset.

**Key Classes and APIs:**

*   **`Orchestrator` (`orchestrator.py`):** This class acts as the central coordinator.
    *   `__init__(self, config: FullConfig)`: The constructor will be dependency-injected with a validated Pydantic configuration object (`FullConfig`). This ensures the orchestrator always operates with a complete and correct set of parameters. It will also initialize the database connection.
    *   `run_cycle_01(self)`: This will be the main public method that executes the entire workflow for this cycle. It will contain the high-level logic to first orchestrate the labelling of all necessary structures and then orchestrate the training of the model.
    *   `_get_structures_to_label(self) -> List[ase.Atoms]`: A private helper method to query the database for all rows with the status 'needs_labelling', returning them as a list of `ase.Atoms` objects.
    *   `_get_labelled_structures(self) -> List[ase.Atoms]`: A similar private method to retrieve the complete dataset (status 'labelled') needed for the training phase.

*   **`LabellingEngine` (`modules/c_labelling_engine.py`):** This class encapsulates all interactions with Quantum Espresso.
    *   `__init__(self, dft_config: DFTParams)`: Its constructor takes only the DFT-specific subset of the configuration, adhering to the principle of least knowledge.
    *   `run(self, atoms: ase.Atoms) -> ase.Atoms`: This is the single public API method. It defines a clear contract: it accepts one `Atoms` object and, upon success, returns the same object, but with the DFT results attached via a calculator. This design makes it easy to test and to use from the orchestrator.
    *   `_generate_qe_input(self, atoms: ase.Atoms) -> str`: A private method responsible for the complex and error-prone task of converting an `Atoms` object and DFT parameters into a valid QE input file string.
    *   `_execute_qe(self, input_str: str) -> Tuple[bool, str, str]`: A private method that wraps the `subprocess` call to `pw.x`. It takes the input string, writes it to the process's stdin, and returns a tuple containing a success flag, the captured stdout, and the captured stderr. This method will contain the core logic for error detection and timeout handling.
    *   `_parse_qe_output(self, output_text: str) -> Dict`: A private method that uses regular expressions to parse the QE output and extract results into a structured dictionary (e.g., `{'energy': -123.4, 'forces': [...]}`).
    *   `_attempt_recovery(self, error_log: str)`: A placeholder for the more advanced error recovery logic to be implemented. Initially, it might just log the error, but it is designed to be extensible.

*   **`TrainingEngine` (`modules/d_training_engine.py`):** This class encapsulates all model training logic.
    *   `__init__(self, train_config: TrainingParams)`: Its constructor takes the training-specific configuration parameters.
    *   `run(self, dataset: List[ase.Atoms])`: The main public method that takes the full labelled dataset and orchestrates the training process, saving the final model to disk.
    *   `_prepare_data(self, dataset: List[ase.Atoms]) -> Any`: A private method to convert the list of `ase.Atoms` objects into the format required by the ACE training library. This is where the delta learning calculation (subtracting the baseline potential's predictions) will occur.
    *   `_train_model(self, formatted_data: Any) -> Any`: A private method that is a direct wrapper around the underlying ACE library's fitting function.
    *   `_save_model(self, trained_model: Any)`: A private method to serialize and save the final trained potential to a file.

**Data Models (`config/models.py` and `data/database.py`):**

Pydantic models will be used extensively to ensure data integrity and provide self-documenting configuration.

*   `DFTParams(BaseModel)`: Will define all parameters related to the Quantum Espresso calculation, such as `command`, `pseudopotentials`, `ecutwfc`, `kpoints_density`, etc. Each field will have a type hint and a docstring explaining its purpose.
*   `TrainingParams(BaseModel)`: Will define parameters for the `TrainingEngine`, including `model_type` (fixed to 'ace' for now), `r_cut`, `delta_learning` (boolean flag), and loss weights.
*   `FullConfig(BaseModel)`: The top-level configuration model that aggregates all other parameter models. The `Orchestrator` will work with this object.
*   The primary "data model" for atomic structures will be the `ase.Atoms` object. The ASE database (`data/database.py`) will store these objects. A custom `AseDB` wrapper class will be created to provide a simplified, application-specific API over the general-purpose `ase.db` functions, including methods like `get_by_status('needs_labelling')` to simplify the orchestrator's logic.

This class-based design ensures strong encapsulation. For example, the `Orchestrator` does not need to know the specifics of how Quantum Espresso is executed; it only needs to adhere to the simple `run` contract of the `LabellingEngine`. This makes the system easier to test, maintain, and extend in future cycles.

## 4. Implementation Approach

The implementation of Cycle 01 will be methodical, starting with the project setup and progressing through each component, with a strong emphasis on a Test-Driven Development (TDD) methodology. This ensures that each piece of the complex workflow is verified before being integrated into the whole.

1.  **Project Scaffolding:**
    *   The first step is to establish a modern, reproducible Python project environment. The project directory structure, as defined in the design architecture, will be created.
    *   The `pyproject.toml` file will be written. This is a critical file that will define the project's name, version, and dependencies. Initial dependencies will include `ase` for atomic structure handling, `pydantic` for configuration modeling, `click` for the future CLI, `uv` for environment management, and the specific Python library required for training ACE potentials.
    *   The development environment will be set up using `uv venv`, and an editable installation will be performed with `uv pip install -e .`. This ensures that any changes to the source code are immediately reflected in the installed package, which is crucial for a smooth development and testing cycle.

2.  **Configuration and Data Models:**
    *   With the project structure in place, the first code to be written will be the Pydantic models in `config/models.py`. This "schema-first" approach establishes a clear, validated data contract that the rest of the application can rely on.
    *   The initial, simple `ConfigExpander` will be implemented in `config/expander.py`. For this cycle, its role is minimal: read a user-provided YAML file and parse it into the `FullConfig` Pydantic model. This step ensures that all subsequent components receive a configuration object that is guaranteed to be complete and type-correct.
    *   The `AseDB` wrapper class will be implemented in `data/database.py`. This class will abstract away the direct calls to the `ase.db` library. It will have methods like `add_structures`, `get_structures_by_status`, and `update_structure_status`, providing a clean, domain-specific API for the orchestrator. Unit tests for this class will be written first, using an in-memory SQLite database to verify its functionality.

3.  **Labelling Engine Implementation (TDD approach):**
    *   The development of the `LabellingEngine` will begin with its most deterministic part: the `_generate_qe_input` method. A unit test will be written first, which asserts that for a given `ase.Atoms` object and `DFTParams`, the method produces a specific, correct QE input string.
    *   Next, the `_parse_qe_output` method will be developed. The test for this method will use a collection of static text files containing sample QE outputs (including successes and various types of failures). The test will assert that the parser correctly extracts the data or raises the appropriate specific exception for each case.
    *   With the input generation and output parsing verified, the `_execute_qe` method will be implemented. Its unit test will use `unittest.mock` to patch `subprocess.run`. The test will verify that the method correctly invokes the QE command and correctly interprets the mocked return values (e.g., a successful return code vs. a non-zero one).
    *   Finally, the public `run` method will be implemented to tie these tested private methods together in the correct sequence. Its integration test will ensure the end-to-end logic of the class is sound.

4.  **Training Engine Implementation (TDD approach):**
    *   Development will start with the `_prepare_data` method, which is the most complex part. The unit test for this method will create a small, fixed list of labelled `ase.Atoms` objects. It will then call the method and assert that the output data structure (which will be formatted for the ACE library) contains the correctly calculated "delta" values. The baseline potential (e.g., Lennard-Jones) will be implemented as a simple, testable utility function.
    *   The `_train_model` and `_save_model` methods will then be implemented. These will likely be thin wrappers around the ACE library's API. Their tests will use mocks to assert that the underlying library functions are called with the correctly prepared data and that the save function is called with the correct filename.

5.  **Orchestrator and CLI:**
    *   With the engines built and unit-tested, the `Orchestrator` class will be implemented. Its `run_cycle_01` method will contain the main workflow logic. Its integration test (described in the Test Strategy section) will be the primary tool to verify its correctness.
    *   Finally, a minimal CLI will be implemented in `cli.py` using `click`. It will provide a single `run` command that takes the path to a configuration file, instantiates the `Orchestrator`, and calls its `run_cycle_01` method. The CLI will be manually tested at this stage to ensure the pipeline can be invoked.

This methodical, test-first approach is crucial for building a reliable and complex scientific workflow, ensuring that each layer of the system is solid before the next is built upon it.

## 5. Test Strategy

The test strategy for Cycle 01 is designed to build confidence in the core components of the pipeline before more complex features are added. It is segmented into unit and integration tests, focusing on correctness, error handling, and the integrity of the data flow. This dual approach ensures that individual functions are correct in isolation and that they also communicate and transfer data correctly when assembled into a larger workflow.

**Unit Testing Approach (Min 300 words):**

The goal of unit tests is to verify the correctness of individual functions and methods in isolation, using mocked inputs and dependencies to eliminate external factors. This allows for fast, targeted testing of business logic.

*   **Configuration Models:** The Pydantic models will be tested for their validation logic. We will write tests that attempt to instantiate the models with both valid and invalid data dictionaries. For example, a test for `DFTParams` would assert that `ValidationError` is raised if a required field like `ecutwfc` is missing, or if a field expecting a float receives a string. This confirms that our configuration system is robust against user error.
*   **Database Wrapper (`AseDB`):** The `AseDB` class will be tested thoroughly using a temporary, in-memory SQLite database (`'test.db', type='sqlite'`). Tests will cover the full CRUD (Create, Read, Update, Delete) lifecycle of a structure. We will write a test that adds several `Atoms` objects with `status='needs_labelling'`, then calls `get_by_status('needs_labelling')` and asserts that the correct number of structures is returned. Another test will verify that the `update_structure_status` method correctly changes the status and saves the new data.
*   **`LabellingEngine`:** This is the most critical component for unit testing due to its interaction with an external process.
    *   `_generate_qe_input`: This method's test will construct a standard `ase.Atoms` object for a material like Silicon and assert that the generated QE input string contains the exact, correct values for `ATOMIC_POSITIONS`, `CELL_PARAMETERS`, and all DFT parameters from a sample `DFTParams` object.
    *   `_parse_qe_output`: A directory of test data will contain sample QE output files. One file will represent a successful run, another a crash, and a third an SCF convergence failure. Tests will call the parser on each of these files and assert that for the successful case, the returned dictionary contains the correct numerical values, and for the failure cases, a specific, informative custom exception (e.g., `SCFConvergenceError`) is raised.
    *   `_execute_qe`: This method's test will use `unittest.mock.patch` to mock `subprocess.run`. We will create different mock return values (e.g., a `CompletedProcess` with `returncode=0`, another with `returncode=1` and error messages on stderr) and assert that `_execute_qe` correctly interprets these mock outcomes into its boolean success flag.
*   **`TrainingEngine`:**
    *   `_prepare_data`: The test for this method is crucial for verifying the delta learning logic. It will create a list containing a single `ase.Atoms` object with a known energy and set of forces. The test will then call `_prepare_data` and inspect the formatted output that would be passed to the ACE library. It will assert that the energy and forces in this output are equal to the original DFT values minus the values calculated by our own simple, verifiable implementation of the baseline potential.

**Integration Testing Approach (Min 300 words):**

Integration tests will verify that the different, unit-tested components of the pipeline work together as intended, focusing on the data flow and state changes managed by the `WorkflowOrchestrator`. These tests are slightly slower as they involve more of the system's components working in concert.

*   **Labelling Workflow Test:** This test will verify the critical loop from the database, to the `LabellingEngine`, and back to the database.
    1.  **Setup:** The test will begin by programmatically creating a temporary ASE database and adding a few (e.g., 3) unlabelled `ase.Atoms` objects.
    2.  **Execution:** It will then instantiate and run the labelling portion of the `Orchestrator`. Crucially, the `subprocess.run` call within the `LabellingEngine` will be mocked. This is essential to prevent the test from needing a real Quantum Espresso installation and from taking a long time. The mock will be configured to return a pre-saved, successful QE output string whenever it is called.
    3.  **Verification:** The test will first assert that the mocked `subprocess.run` was called exactly 3 times. After the orchestrator's method completes, the test will query the temporary database and assert that all 3 structures now have the status 'labelled'. It will also load one of the `Atoms` objects from the database and assert that it now contains the correct, parsed energy and force data from the mocked output. This validates the entire data flow for the labelling stage.
*   **Full Cycle Test (Mocked DFT):** This will be the main integration test for the cycle, covering the entire workflow from start to finish.
    1.  **Setup:** Similar to the above, the test will start with a database of unlabelled structures. A full configuration object will be created.
    2.  **Execution:** The test will call the main `Orchestrator.run_cycle_01` method. The `LabellingEngine` will be mocked as described above. Additionally, the `TrainingEngine`'s underlying call to the ACE library's `fit` function will also be mocked. This mock will be configured to simply check that it receives correctly formatted data and to create a dummy output file to simulate a successful training run.
    3.  **Verification:** The test will assert that the workflow completes without any exceptions. It will then verify the chain of state transitions in the database (from 'needs_labelling' to 'labelled'). It will assert that the mock for the ACE training function was called exactly once and with the expected data (i.e., the data for all the successfully labelled structures). Finally, it will assert that a dummy output file for the trained potential (e.g., `model.ace`) has been created in the expected location. This test provides high confidence that the entire chain of command, from orchestrator to engines and back to the database, is functioning correctly.
