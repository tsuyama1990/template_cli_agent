# SPEC.md: Cycle 04 - The Active Learning Loop

## 1. Summary

Cycle 04 represents the technical and conceptual apex of the MLIP-AutoPipe project. It is in this cycle that we implement the final and most critical component required for fully autonomous operation: the **Simulation Engine (Module E)**. This module is not merely a tool for running simulations; it is the engine that drives a closed-loop, on-the-fly (OTF) active learning cycle. The primary and ambitious goal of this cycle is to transform the pipeline from the static, linear, feed-forward process developed in the preceding cycles into a dynamic, intelligent, and self-improving system. This cycle is the literal embodiment of the project's core philosophy to "remove the human expert from the loop." It achieves this by empowering the system to prospectively identify the weaknesses and limitations of its own MLIP model during a live simulation, and then to autonomously take the necessary steps to correct those deficiencies.

The central feature to be developed in this cycle is the implementation of an MLIP-driven simulation engine that can run standard molecular dynamics (MD) while simultaneously performing real-time uncertainty quantification. As the simulation evolves, the engine will constantly monitor the uncertainty of the MLIP's predictions for the forces on each atom. When the simulation trajectory enters a region of the potential energy surface where the model is uncertain—an "extrapolation domain" where its predictions are unreliable—the engine will automatically and gracefully pause the simulation. It will then execute a sophisticated procedure to extract the novel atomic configuration, send it back to the **Labelling Engine (Module C)** for high-fidelity DFT calculation, and trigger a retraining of the model. The newly labelled data point is then used to retrain or update the MLIP, systematically improving its accuracy and robustness. This iterative process of "simulate, detect, label, retrain, resume" allows the potential to systematically and efficiently expand its domain of validity, becoming more reliable, more accurate, and more transferable with each completed cycle.

This cycle presents several significant technical challenges that must be overcome. These include the implementation of a robust, computationally efficient, and physically meaningful uncertainty quantification metric suitable for the chosen MLIP architecture. Furthermore, we must develop a dynamic thresholding mechanism for triggering the retraining process, a mechanism that can adapt to the model's growing maturity. Finally, a sophisticated and non-trivial method for extracting local atomic environments from bulk periodic simulations must be designed to ensure that the structures sent for DFT labelling are physically meaningful and free from boundary condition artefacts. Upon the successful completion of Cycle 04, the MLIP-AutoPipe will have matured into a fully autonomous learning machine, capable of starting with a nascent, minimally-trained potential and iteratively and systematically refining it until it is robust enough for production-level scientific simulations.

## 2. System Architecture

The architecture in Cycle 04 undergoes a fundamental transformation, evolving from a linear pipeline into a closed-loop, cyclical system. This is the most significant architectural change in the project. The `Simulation Engine (Module E)` is introduced as the new primary consumer of the trained potential generated by Module D. Critically, however, it also becomes a new and powerful source of high-value structures for the `Labelling Engine (Module C)`, thus "closing the loop".

The detailed workflow of the active learning loop is as follows:
1.  **Initial Model Training**: The overall process begins with the execution of the full workflow defined in Cycles 1-3. This results in an initial, baseline MLIP being trained on a diverse set of structures. This model is then stored in the database and designated as the starting point for the active learning phase.
2.  **Initiate OTF Simulation**: The `Orchestrator`, having completed the initial training, now transitions the system into the active learning state. It starts the `Simulation Engine (Module E)`, loading the baseline MLIP from the database.
3.  **MLIP-Driven MD**: Module E initiates a molecular dynamics simulation using the loaded MLIP as the calculator. At each timestep (or every few steps, as a performance trade-off), it calculates not only the energy and forces required to propagate the dynamics but also an uncertainty metric associated with the force prediction for the current atomic configuration.
4.  **Uncertainty Monitoring and Comparison**: The calculated uncertainty metric is then compared against a **Dynamic Uncertainty Threshold**. This threshold is a critical parameter. It is not a fixed, magic value but is statistically determined from the distribution of uncertainties on the *existing* training set. This makes the triggering mechanism adaptive; as the model improves, the threshold naturally tightens.
5.  **Extrapolation Event Detected**: If the calculated uncertainty for the current frame exceeds this dynamic threshold, it is flagged as an "extrapolation event," and the MD simulation is gracefully paused.
6.  **Sophisticated Structure Extraction**: The atomic configuration that triggered the high uncertainty is then carefully extracted. This is a non-trivial process that involves a sophisticated procedure to correctly handle periodic boundary conditions, creating a physically meaningful, charge-neutral, and isolated cluster (e.g., by including a buffer region and applying force masking techniques). This ensures the subsequent DFT calculation is well-posed.
7.  **Submission for Labelling**: The extracted structure is then saved to the `AseDB` with a specific state, such as 'active_learning_candidate', clearly marking its origin and high value.
8.  **Retraining Trigger by Orchestrator**: The `Orchestrator` detects that a new candidate structure is available for labelling. It retrieves this structure and passes it to the `Labelling Engine (Module C)` for the expensive but necessary DFT calculation.
9.  **Model Update/Retraining**: Once the structure has been successfully labelled, the new data point (structure and DFT results) is added to the training set. The `Orchestrator` then invokes the `Training Engine (Module D)` to either retrain the MLIP from scratch with the augmented dataset or, if the framework supports it, perform a more efficient update.
10. **Resume Simulation with Improved Model**: The `Simulation Engine` is then restarted. It loads the newly improved MLIP and can either resume the simulation from the point where it was paused or start a new simulation. The entire process then continues from step 3.

This circular flow of data is the defining architectural pattern of the entire project. The system is no longer a simple, static pipeline but a true, dynamic learning loop. The `Orchestrator` plays the most critical role in this cycle, managing this stateful, cyclical workflow, and ensuring that simulations are paused, data is routed correctly between the different engines, models are updated, and the simulation is resumed seamlessly and robustly.

## 3. Design Architecture

The design for Cycle 04 will concentrate on the implementation of the new `SimulationEngine` class (Module E) and a significant enhancement of the `Orchestrator` to manage the complex, stateful logic of the active learning loop. The design must be robust enough to handle many iterations of this loop without failure.

**New and Updated Classes/APIs:**

1.  **`mlip_autopipec.modules.simulation_engine.SimulationEngine`** (New Class, in `modules/simulation_engine.py`)
    *   **Purpose**: To run MLIP-driven simulations with integrated, on-the-fly uncertainty quantification, and to handle the extraction of high-uncertainty structures.
    *   **Public API**:
        *   `__init__(config: FullConfig)`: The constructor will take the full configuration object to access MD parameters and uncertainty settings.
        *   `run_otf_md(mlip_model: Any, uncertainty_threshold: float) -> Optional[ase.Atoms]`: The main entry point for an active learning simulation. It will run an MD simulation until a high-uncertainty structure is found, at which point it will extract it and return it. If the simulation completes without exceeding the threshold, it will return `None`.
        *   `get_uncertainty(atoms: ase.Atoms) -> float`: A method to calculate the uncertainty for a given configuration. The exact implementation is highly dependent on the MLIP model. For committee models, such as some variants of ACE, this method will calculate the standard deviation of the force predictions from the different committee members.
    *   **Key Internal Methods**:
        *   `_extract_structure_for_labelling(atoms: ase.Atoms, high_uncertainty_atom_index: int) -> ase.Atoms`: This private method implements the advanced logic for correctly handling periodic boundary conditions. It will identify the atom with the highest uncertainty, define a "core" region around it, add a larger "buffer" region, and create a new, isolated `ase.Atoms` object that is suitable for a clean, artefact-free DFT calculation.

2.  **`mlip_autopipec.modules.training_engine.TrainingEngine`** (Updated)
    *   **New Method**: `update(model: Any, new_data: List[...]) -> Any`: An optional, more efficient method for updating an existing model with new data points. This is only feasible if the underlying MLIP framework supports online learning or efficient updates. If not, the existing `train` method will be used for a full retraining on the augmented dataset in each cycle.

3.  **`mlip_autopipec.orchestrator.Orchestrator`** (Updated)
    *   **New Method**: `run_active_learning_loop(max_iterations: int)`: This will become the new top-level method for the main execution phase of the pipeline, encapsulating the entire iterative learning process.
    *   **New Method**: `_calculate_dynamic_threshold(model: Any) -> float`: A private method that encapsulates the logic for determining the uncertainty threshold for the next iteration. It will fetch the current training set, calculate the uncertainty of the model on all of them, and then compute a high percentile (e.g., the 99th percentile) to use as the new threshold.
    *   **Updated Workflow Logic**: The `run_active_learning_loop` method will contain the main `while` or `for` loop for the active learning process. The pseudocode for this critical logic is:
        ```python
        # Pseudocode for the loop in Orchestrator
        for i in range(max_iterations):
            latest_mlip = db.get_latest_model()
            threshold = self._calculate_dynamic_threshold(latest_mlip)

            high_uncertainty_atoms = simulation_engine.run_otf_md(latest_mlip, threshold)

            if high_uncertainty_atoms is None:
                log("Simulation completed without finding new uncertain structures. MLIP is now robust.")
                break

            db.add_atoms(high_uncertainty_atoms, state='active_learning_candidate')
            self.run_labelling_phase()  # Re-uses logic from Cycle 1
            self.run_training_phase()   # Re-uses logic from Cycle 1
        ```

**Detailed Uncertainty Quantification Strategy:**
The choice of uncertainty metric is intrinsically tied to the MLIP model architecture. The planned approach is to use a committee of ACE models. During the training phase, instead of training one model, we will train a small ensemble (e.g., 3-5 models) on the same data but with different random initialisations. The `get_uncertainty` method will then calculate the forces for a given `Atoms` object with each model in the committee. The uncertainty will be defined as the trace of the covariance matrix of the force vectors, which is a robust and widely-used metric for uncertainty in atomistic simulations.

**Detailed Structure Extraction Logic:**
The `_extract_structure_for_labelling` method is one of the most technically challenging parts of this cycle. Simply cutting a spherical cluster out of a periodic simulation can create unphysical dangling bonds or charge imbalances that corrupt the subsequent DFT calculation. The implementation must follow a robust protocol:
1.  Identify the specific atom that has the highest force uncertainty.
2.  Define a spherical "core" region (e.g., with a radius of 5 Angstroms) centered on this atom.
3.  Define a larger spherical "buffer" region (e.g., with a radius of 8 Angstroms) around the same central atom.
4.  Create a new, isolated (non-periodic) `ase.Atoms` object that contains all atoms from both the core and buffer regions.
5.  During the subsequent DFT labelling and MLIP training steps, the forces on the buffer atoms will be completely ignored (masked out from the loss function). This ensures that only the physically reliable forces on the well-solvated core region atoms contribute to the model's training.

## 4. Implementation Approach

The implementation for Cycle 04 will be methodical, focusing on building the core simulation and uncertainty logic first, then developing the sophisticated data handling methods, and finally wrapping everything in the new orchestration logic required for the active learning loop.

1.  **Uncertainty Metric Implementation**:
    *   The first step is a technical deep-dive into the chosen ACE library to confirm its capabilities for training committee models. The `TrainingEngine` will be updated to support training an ensemble of models instead of just one.
    *   With this in place, the `get_uncertainty` method in the `SimulationEngine` will be implemented. This will involve iterating through each model in the committee, calling its prediction function to get the forces, and then computing the standard deviation or a related metric across the ensemble's predictions.

2.  **Develop the Core Simulation Engine (Module E)**:
    *   The main `run_otf_md` method will be implemented. This will use a standard ASE MD integrator, such as `VelocityVerlet`.
    *   The core of this method will be a custom Python `for` loop over the simulation steps, rather than a simple call to `dyn.run()`. This is essential to allow for the injection of our custom logic at each step.
    *   Inside this loop, after each `dyn.step()` call, the engine will call its own `get_uncertainty` method.
    *   It will then compare the result to the provided threshold. If the threshold is exceeded, the loop will be broken, and the method will proceed to the structure extraction phase before returning the problematic `Atoms` object.

3.  **Dynamic Threshold and Structure Extraction Logic**:
    *   The `_calculate_dynamic_threshold` method will be implemented in the `Orchestrator`. This function will be called at the beginning of each active learning iteration. It will need to fetch the entire current training set from the `AseDB`, calculate the uncertainty for each data point using the latest committee model, and then compute a high statistical percentile (e.g., `np.percentile(uncertainties, 99)`). This value will be passed to the `SimulationEngine` for the next MD run.
    *   The `_extract_structure_for_labelling` method in the `SimulationEngine` will be implemented according to the detailed protocol described in the Design Architecture. This is a complex task that involves careful handling of atomic indices, positions, and periodic boundary conditions using ASE's helper functions.

4.  **Orchestrator Loop Implementation**:
    *   The `Orchestrator` class will be significantly refactored to implement the main `run_active_learning_loop` method.
    *   This is the most critical part of the integration. The method needs to flawlessly manage the state of the system across many iterations: loading the latest model, calculating the new threshold, calling Module E, checking the result, and then conditionally calling Module C and Module D before repeating the loop. Robust logging at each step will be essential for debugging.

5.  **Final Integration**:
    *   The final step is to integrate the full, end-to-end loop. A complete run of the system will now involve all modules working in concert in a complex, cyclical fashion: A -> B -> C -> D -> E -> C -> D -> E ..., which represents the culmination of the project's core technical goals.

This layered approach ensures that the core scientific logic (uncertainty, extraction) is built and unit-tested first, before being wrapped in the stateful, iterative logic of the `Orchestrator`.

## 5. Test Strategy

Testing Cycle 04 presents a significant challenge due to its dynamic, stateful, and cyclical nature. The strategy will therefore rely on a combination of carefully controlled and isolated unit tests for the complex new logic, and a sophisticated "toy model" integration test to verify the behavior of the entire feedback loop in a controlled and observable environment.

**Unit Testing Approach (Min 600 words):**

The unit tests for this cycle will be designed to rigorously validate the new, complex algorithms in the `SimulationEngine` and the new control flow in the `Orchestrator`.

*   **`SimulationEngine`**: This class will be tested in complete isolation by providing it with a mock MLIP model. The mock will be highly configurable to allow for the testing of different scenarios.
    *   The `get_uncertainty` method will be tested by providing a mock model that returns a known set of force vectors from its "committee members." The test will then assert that the calculated standard deviation is arithmetically correct.
    *   The main `run_otf_md` method will be tested by programming the mock MLIP to behave in a specific, deterministic way. For example, the mock will be configured to return a low uncertainty value for the first 4 simulation steps, and then a very high uncertainty value on step 5. The test will then assert that the MD loop runs for exactly 5 steps, that it correctly identifies the high-uncertainty event, and that it returns the correct `Atoms` object from that specific step.
    *   The `_extract_structure_for_labelling` method is critical to test. A known, periodic `ase.Atoms` object (e.g., a 3x3x3 supercell of Aluminum) will be created. The test will call the extraction method with a specific atom index. It will then perform a series of assertions on the returned, non-periodic structure: verify that the periodic boundary condition flags are all `False`, verify that the number of atoms in the returned object is correct and corresponds to the expected number of core and buffer atoms, and verify that the atomic positions have been correctly mapped from the periodic cell to the new, isolated cluster.

*   **`Orchestrator`**: The `run_active_learning_loop` method will be tested by completely mocking all the engine modules (A, B, C, D, and E). The purpose of this test is not to check the engines, but to verify the orchestrator's control flow and state management logic.
    *   The test will be configured to simulate a two-iteration run. The mock `SimulationEngine` will be configured to return a "high uncertainty" structure on its first call, and then to return `None` (indicating a successful, completed run) on its second call.
    *   The test will then assert that the `LabellingEngine.run` method was called exactly once, and that the `TrainingEngine.train` method was also called exactly once. This verifies that a single retraining cycle was correctly triggered. The test will also assert that the `SimulationEngine.run_otf_md` method was called a total of two times, proving that the loop ran, correctly identified the need to retrain, and then terminated correctly on the subsequent iteration.

**Integration Testing Approach (Min 300 words):**

A full integration test with real DFT calculations and real, multi-hour MLIP training is completely infeasible for an automated test suite. Therefore, a sophisticated "toy problem" integration test will be developed. This test is designed to verify the emergent behavior of the entire feedback loop in a simplified, controlled, and fast-running environment, using mocked external processes.

The test setup will be as follows:
1.  **Toy Potential**: A simple, 2D analytical potential will be created in pure Python (e.g., a function with two minima and a known saddle point between them). This function will serve as our "ground truth" reality.
2.  **Mock `LabellingEngine`**: The `LabellingEngine` will be mocked so that its `run` method does not call DFT. Instead, it will be a lightweight "mock" that simply evaluates the toy potential's true energy and analytical gradients (forces) for any given input coordinate.
3.  **Real `TrainingEngine`**: The `TrainingEngine` will be real, but instead of training a complex ACE model, it will be configured to train a very simple MLIP (such as a small neural network or a Gaussian process model) on the 2D toy potential data.
4.  **Real `SimulationEngine`**: The `SimulationEngine` will be real. It will run a real 2D Molecular Dynamics simulation on the potential energy surface predicted by the simple, 2D MLIP. Its uncertainty metric can be a simple but effective proxy, such as the distance of the current MD coordinate from the nearest existing training point.

The test will then execute the full `Orchestrator.run_active_learning_loop` on this toy system:
1.  The loop will be started with only one or two training points provided to the MLIP, meaning its initial approximation of the 2D surface will be very poor.
2.  The test will then run the loop for a fixed number of iterations. The `SimulationEngine`'s MD will explore the 2D space. As it moves far away from the initial training points, the uncertainty metric will become high, triggering the loop.
3.  The test will assert that the loop correctly pauses the simulation, "labels" the new point using the mock `LabellingEngine` (i.e., gets the true value from the analytical potential), and retrains the 2D MLIP.
4.  The results can even be visualised by plotting the 2D potential and the sequence of points chosen for labelling. We should observe the system intelligently and iteratively adding new training points in the areas where the MLIP's error is highest. This provides extremely high confidence in the correctness of the overall active learning architecture and its emergent behavior.
