from typing import Any

from ac_cdd_core.agents import get_model
from ac_cdd_core.config import settings
from ac_cdd_core.domain_models import PlanAuditResult
from pydantic_ai import Agent
from rich.console import Console

console = Console()


class PlanAuditor:
    """
    Audits implementation plans generated by Jules against project specifications.
    Uses a Smart Model (Auditor) to ensure safety and alignment.
    """

    def __init__(self):
        # Use the configured SMART_MODEL (Auditor)
        # Note: The prompt instructed to use SMART_MODEL (Upper Level LLM)
        # settings.reviewer.smart_model maps to SMART_MODEL env var or default
        self.model_name = settings.reviewer.smart_model
        self.model = get_model(self.model_name)

        self.agent = Agent(
            model=self.model,
            result_type=PlanAuditResult,
            retries=3,
            system_prompt=(
                "You are an expert Software Architect and QA Manager.\n"
                "Review the implementation plan provided by the AI Agent (Jules) against the "
                "project specifications strictly.\n\n"
                "Criteria:\n"
                "1. Completeness: Does it cover all requirements in SPEC/UAT?\n"
                "2. Safety: Does it avoid destroying unrelated files or adding unnecessary deps?\n"
                "3. Consistency: Does it follow ARCHITECT_INSTRUCTION?\n"
                "4. Specificity: Are steps actionable?\n\n"
                "Output MUST be a structured JSON object matching the result type."
            ),
        )

    async def audit_plan(
        self, plan_details: dict[str, Any], spec_context: dict[str, str]
    ) -> PlanAuditResult:
        """
        Evaluates a plan against the provided specification context.
        """
        # 1. Construct the Prompt
        specs_text = ""
        for name, content in spec_context.items():
            specs_text += f"\n--- {name} ---\n{content}\n"

        plan_text = f"Plan ID: {plan_details.get('planId', 'N/A')}\nSteps:\n"
        for step in plan_details.get("steps", []):
            step_id = step.get("id", "-")
            desc = step.get("description", "")
            plan_text += f"- [{step_id}] {desc}\n"

        user_prompt = (
            f"### Project Specifications\n{specs_text}\n\n"
            f"### Proposed Implementation Plan\n{plan_text}\n\n"
            "Evaluate this plan. If REJECTED, provide specific imperative feedback."
        )

        # 2. Run the Agent
        console.print(f"[dim]Auditing plan using {self.model_name}...[/dim]")
        result = await self.agent.run(user_prompt)

        return result.data
