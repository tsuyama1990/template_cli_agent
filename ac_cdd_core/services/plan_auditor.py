from typing import Any

from ac_cdd_core.agents import get_model
from ac_cdd_core.config import settings
from ac_cdd_core.domain_models import PlanAuditResult
from pydantic_ai import Agent
from rich.console import Console

console = Console()


class PlanAuditor:
    """
    Audits implementation plans generated by Jules against project specifications.
    Uses a Smart Model (Auditor) to ensure safety and alignment.
    """

    def __init__(self):
        # Use the configured SMART_MODEL (Auditor)
        # Note: The prompt instructed to use SMART_MODEL (Upper Level LLM)
        # settings.reviewer.smart_model maps to SMART_MODEL env var or default
        self.model_name = settings.reviewer.smart_model
        self.model = get_model(self.model_name)

        # Fix for Pydantic AI usage: result_type matches the generic type parameter if provided,
        # or should be passed correctly.
        # The error `Unknown keyword arguments: result_type` implies this version of pydantic-ai
        # Agent class doesn't take result_type in __init__ kwargs directly in this way,
        # or it should be strictly typed.
        # Actually, pydantic-ai Agent signature usually is Agent(model, system_prompt, result_type=...).
        # If it complains about kwargs, maybe it's being passed to a superclass incorrectly?

        # Let's try explicitly typing the Agent generic or check if result_type is valid.
        # Assuming standard pydantic-ai, result_type IS valid.
        # The traceback showed `pydantic_ai.exceptions.UserError: Unknown keyword arguments: result_type`
        # inside `validate_empty_kwargs`. This usually happens when arguments are passed that aren't expected.

        # NOTE: In some versions, it might be `result_schema` or handled via generic `Agent[Deps, ResultType]`.
        # However, checking the library docs (simulated), `result_type` is standard.
        # Maybe the installed version is very new or old?
        # Traceback said: .venv/lib/python3.12/site-packages/pydantic_ai/_utils.py:467

        # To be safe and fix the test, I will remove `result_type` from init and rely on `run(..., result_type=PlanAuditResult)`
        # OR just define it via Generic if possible.
        # But `run` method usually takes result_type override.
        # Let's try passing it to run() instead, or rely on the fact that we might need to cast it.

        # Actually, looking at `tests/ac_cdd/unit/test_graph.py`, the error happens during instantiation of PlanAuditor.
        # I will remove result_type from __init__ and pass it to run() if supported,
        # or simply ignore it if I can't fix the library mismatch, but strictly it should work.

        # Let's try:
        self.agent = Agent(
            model=self.model,
            # result_type=PlanAuditResult,  <-- Removing this from init to fix the error
            retries=3,
            system_prompt=(
                "You are an expert Software Architect and QA Manager.\n"
                "Review the implementation plan provided by the AI Agent (Jules) against the "
                "project specifications strictly.\n\n"
                "Output MUST be a structured JSON object matching the result type."
            ),
        )

    async def audit_plan(
        self, plan_details: dict[str, Any], spec_context: dict[str, str]
    ) -> PlanAuditResult:
        """
        Evaluates a plan against the provided specification context.
        """
        # 1. Construct the Prompt
        specs_text = ""
        for name, content in spec_context.items():
            specs_text += f"\n--- {name} ---\n{content}\n"

        plan_text = f"Plan ID: {plan_details.get('planId', 'N/A')}\nSteps:\n"
        for step in plan_details.get("steps", []):
            step_id = step.get("id", "-")
            desc = step.get("description", "")
            plan_text += f"- [{step_id}] {desc}\n"

        user_prompt = (
            f"### Project Specifications\n{specs_text}\n\n"
            f"### Proposed Implementation Plan\n{plan_text}\n\n"
            "Evaluate this plan. If REJECTED, provide specific imperative feedback."
        )

        # 2. Run the Agent
        console.print(f"[dim]Auditing plan using {self.model_name}...[/dim]")
        # Pass result_type here to enforce structure
        result = await self.agent.run(user_prompt, result_type=PlanAuditResult)

        return result.data
